{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e7a8b6-b5a1-4f30-bdf8-5e5129477044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airflow Integration Parameters\n",
    "\n",
    "try:\n",
    "    # Create widgets for Airflow parameters\n",
    "    dbutils.widgets.text(\"batch_id\", \"manual_run\", \"Batch ID from Airflow\")\n",
    "    dbutils.widgets.text(\"execution_date\", \"\", \"Execution Date from Airflow\") \n",
    "    dbutils.widgets.text(\"force_refresh\", \"false\", \"Force data refresh\")\n",
    "    dbutils.widgets.text(\"quality_threshold\", \"0.8\", \"Data quality threshold\")\n",
    "    dbutils.widgets.text(\"dag_run_id\", \"\", \"DAG Run ID\")\n",
    "    \n",
    "    # Get parameter values\n",
    "    batch_id = dbutils.widgets.get(\"batch_id\")\n",
    "    execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "    force_refresh = dbutils.widgets.get(\"force_refresh\").lower() == \"true\"\n",
    "    quality_threshold = float(dbutils.widgets.get(\"quality_threshold\"))\n",
    "    dag_run_id = dbutils.widgets.get(\"dag_run_id\")\n",
    "    \n",
    "    print(f\"\uD83C\uDFAF Airflow Parameters:\")\n",
    "    print(f\"   Batch ID: {batch_id}\")\n",
    "    print(f\"   Execution Date: {execution_date}\")\n",
    "    print(f\"   Force Refresh: {force_refresh}\")\n",
    "    print(f\"   Quality Threshold: {quality_threshold}\")\n",
    "    print(f\"   DAG Run ID: {dag_run_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Widget creation failed (normal in some contexts): {e}\")\n",
    "    # Fallback values for manual runs\n",
    "    batch_id = \"manual_run\"\n",
    "    execution_date = \"\"\n",
    "    force_refresh = False\n",
    "    quality_threshold = 0.8\n",
    "    dag_run_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6711885e-e735-4875-b1ce-9813fa388b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Data Consumer Script\n======================================================================\n\uD83C\uDFD7️ Setting up database schemas with ALL STRING types...\n\uD83D\uDD0D Checking existing schemas...\n\uD83D\uDCC1 Current catalog: databricks_stock_sentiment_canada\n\uD83D\uDCC1 Current database: default\n\uD83D\uDCCB Schema columns available: ['databaseName']\n\uD83D\uDCCB Existing schemas: ['bronze', 'default', 'information_schema']\n\uD83D\uDD27 Creating silver schema...\n✅ Silver schema created\n✅ Bronze schema already exists\n\uD83D\uDDD1️ Dropped any existing consumer table\n✅ Table databricks_stock_sentiment_canada.silver.stock_data_consumer created with ALL STRING types\n\uD83D\uDCCB Available tables in silver schema: ['stock_data_consumer']\n\n======================================================================\n\uD83D\uDCC8 Running Stock Data Consumer\n======================================================================\n✅ Configuration loaded\n\uD83D\uDE80 Starting Stock Consumer Script\n⏰ 2025-07-21 22:12:10\n\n\uD83D\uDD04 Starting stock data consumption and processing...\n\uD83D\uDCC8 1: STOCK - AAPL @ $211.18\n\uD83D\uDCC8 2: STOCK - GOOGL @ $185.06\n\uD83D\uDCC8 3: STOCK - MSFT @ $510.05\n\uD83D\uDCC8 4: STOCK - AMZN @ $226.13\n\uD83D\uDCC8 5: STOCK - META @ $704.28\n\uD83D\uDCC8 6: STOCK - TSLA @ $329.65\n✅ Consumed 6 stock messages\n\n\uD83D\uDD0D DEBUG - Message 1 structure:\n   Keys: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   data_type: stock_price\n   symbol: AAPL\n   symbol: AAPL (type: str)\n   timestamp: 2025-07-18T20:00:00 (type: str)\n   open_price: 210.87 (type: str)\n   high_price: 211.79 (type: str)\n   low_price: 209.7045 (type: str)\n   close_price: 211.18 (type: str)\n   volume: 48974591.0 (type: str)\n   source: polygon.io (type: str)\n   data_type: stock_price (type: str)\n   market_cap_estimate: 10342454127.380001 (type: str)\n   daily_range: 2.085499999999996 (type: str)\n   api_fetch_time: 2025-07-21T22:09:51.018098+00:00 (type: str)\n   event_timestamp: 2025-07-21T22:10:52.649148+00:00 (type: str)\n   event_source: api_producer (type: str)\n   partition_key: AAPL (type: str)\n   processing_mode: streaming (type: str)\n   consumer_timestamp: 2025-07-21T22:12:17.787432+00:00 (type: str)\n   partition_id: 3 (type: str)\n\n\uD83D\uDCC8 Processing 6 stock events:\n\n\uD83D\uDD0D Processing stock 1: AAPL\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: AAPL\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 211.18 -> 211.18\n   open_price: 210.87 -> 210.87\n   high_price: 211.79 -> 211.79\n   low_price: 209.7045 -> 209.7045\n   volume: 48974591.0 -> 48974591\n   Final prices - O:210.87, H:211.79, L:209.7045, C:211.18, V:48974591\n✅ Stock 1: AAPL @ $211.18 processed successfully\n\n\uD83D\uDD0D Processing stock 2: GOOGL\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: GOOGL\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 185.06 -> 185.06\n   open_price: 185.4 -> 185.4\n   high_price: 186.42 -> 186.42\n   low_price: 183.71 -> 183.71\n   volume: 34014509.0 -> 34014509\n   Final prices - O:185.4, H:186.42, L:183.71, C:185.06, V:34014509\n✅ Stock 2: GOOGL @ $185.06 processed successfully\n\n\uD83D\uDD0D Processing stock 3: MSFT\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: MSFT\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 510.05 -> 510.05\n   open_price: 514.48 -> 514.48\n   high_price: 514.64 -> 514.64\n   low_price: 507.43 -> 507.43\n   volume: 21209666.0 -> 21209666\n   Final prices - O:514.48, H:514.64, L:507.43, C:510.05, V:21209666\n✅ Stock 3: MSFT @ $510.05 processed successfully\n\n\uD83D\uDD0D Processing stock 4: AMZN\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: AMZN\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 226.13 -> 226.13\n   open_price: 225.14 -> 225.14\n   high_price: 226.4 -> 226.4\n   low_price: 222.98 -> 222.98\n   volume: 37833807.0 -> 37833807\n   Final prices - O:225.14, H:226.4, L:222.98, C:226.13, V:37833807\n✅ Stock 4: AMZN @ $226.13 processed successfully\n\n\uD83D\uDD0D Processing stock 5: META\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: META\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 704.28 -> 704.28\n   open_price: 702.19 -> 702.19\n   high_price: 704.71 -> 704.71\n   low_price: 691.65 -> 691.65\n   volume: 12779752.0 -> 12779752\n   Final prices - O:702.19, H:704.71, L:691.65, C:704.28, V:12779752\n✅ Stock 5: META @ $704.28 processed successfully\n\n\uD83D\uDD0D Processing stock 6: TSLA\n   Available fields: ['symbol', 'timestamp', 'open_price', 'high_price', 'low_price', 'close_price', 'volume', 'source', 'data_type', 'market_cap_estimate', 'daily_range', 'api_fetch_time', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   Symbol: TSLA\n   Timestamp: 2025-07-18T20:00:00\n   close_price: 329.65 -> 329.65\n   open_price: 321.66 -> 321.66\n   high_price: 330.9 -> 330.9\n   low_price: 321.42 -> 321.42\n   volume: 94254993.0 -> 94254993\n   Final prices - O:321.66, H:330.9, L:321.42, C:329.65, V:94254993\n✅ Stock 6: TSLA @ $329.65 processed successfully\n\n\uD83D\uDCBE Saving 6 processed stocks...\n✅ Saved stocks to Silver layer (ADLS)\n✅ Also saved stocks to Unity Catalog\n\n\uD83D\uDD0D Verification:\n✅ Silver layer stock files: 2\n✅ Verified: 6 new stock records (6 total)\n\n\uD83D\uDCCA Sample Stock Records:\n+------+-----------+--------+----------+----------------+--------------------------------+\n|symbol|close_price|volume  |volatility|volume_indicator|silver_processing_time          |\n+------+-----------+--------+----------+----------------+--------------------------------+\n|TSLA  |329.65     |94254993|0.028758  |high            |2025-07-21T22:12:33.248727+00:00|\n|META  |704.28     |12779752|0.018544  |low             |2025-07-21T22:12:33.248617+00:00|\n|AMZN  |226.13     |37833807|0.015124  |medium          |2025-07-21T22:12:33.248538+00:00|\n|MSFT  |510.05     |21209666|0.014136  |medium          |2025-07-21T22:12:33.248436+00:00|\n|GOOGL |185.06     |34014509|0.014644  |medium          |2025-07-21T22:12:33.248327+00:00|\n+------+-----------+--------+----------+----------------+--------------------------------+\n\n\n\uD83D\uDCC8 Stock Data Summary:\n+------+------------+\n|symbol|record_count|\n+------+------------+\n|AAPL  |1           |\n|AMZN  |1           |\n|GOOGL |1           |\n|META  |1           |\n|MSFT  |1           |\n|TSLA  |1           |\n+------+------------+\n\n\n\uD83D\uDCCB Final Summary:\n✅ Consumed: 6 stock messages\n✅ Processed: 6 stock records\n✅ ADLS save: True\n\uD83D\uDCC1 Batch ID: stock_consumer_20250721_221233\n\uD83C\uDFAF Focus: Stock data with ALL STRING types (no conflicts possible)\n\uD83D\uDCCB New Table: databricks_stock_sentiment_canada.silver.stock_data_consumer\n\n⏰ COMPLETED: 22:12:40\n\uD83C\uDFAF Stock data consumer with ALL STRING types\n"
     ]
    }
   ],
   "source": [
    "print(\"Stock Data Consumer Script\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Store Python's built-in round before PySpark imports override it\n",
    "import builtins\n",
    "python_round = builtins.round\n",
    "\n",
    "# Now import other modules\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "from azure.eventhub import EventHubConsumerClient, TransportType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import traceback\n",
    "\n",
    "# Enhanced schema setup with ALL STRING types to prevent ANY conflicts\n",
    "print(\"\uD83C\uDFD7️ Setting up database schemas with ALL STRING types...\")\n",
    "\n",
    "def create_schemas_and_tables():\n",
    "    \"\"\"Create catalog, schema, and tables for stock data - ALL STRING TYPES\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\uD83D\uDD0D Checking existing schemas...\")\n",
    "        \n",
    "        # Get current catalog name\n",
    "        current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "        current_database = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "        \n",
    "        print(f\"\uD83D\uDCC1 Current catalog: {current_catalog}\")\n",
    "        print(f\"\uD83D\uDCC1 Current database: {current_database}\")\n",
    "        \n",
    "        # Show existing schemas with improved column access\n",
    "        try:\n",
    "            schemas_df = spark.sql(\"SHOW SCHEMAS\")\n",
    "            schema_columns = schemas_df.columns\n",
    "            print(f\"\uD83D\uDCCB Schema columns available: {schema_columns}\")\n",
    "            \n",
    "            if 'namespace' in schema_columns:\n",
    "                existing_schemas = [row['namespace'] for row in schemas_df.collect()]\n",
    "            elif 'databaseName' in schema_columns:\n",
    "                existing_schemas = [row['databaseName'] for row in schemas_df.collect()]\n",
    "            elif 'schemaName' in schema_columns:\n",
    "                existing_schemas = [row['schemaName'] for row in schemas_df.collect()]\n",
    "            else:\n",
    "                existing_schemas = [row[0] for row in schemas_df.collect()]\n",
    "            \n",
    "            print(f\"\uD83D\uDCCB Existing schemas: {existing_schemas}\")\n",
    "            \n",
    "        except Exception as schema_list_error:\n",
    "            print(f\"⚠️ Could not list schemas: {schema_list_error}\")\n",
    "            existing_schemas = []\n",
    "        \n",
    "        # Create silver schema if it doesn't exist\n",
    "        if 'silver' not in existing_schemas:\n",
    "            print(\"\uD83D\uDD27 Creating silver schema...\")\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.silver\")\n",
    "            print(\"✅ Silver schema created\")\n",
    "        else:\n",
    "            print(\"✅ Silver schema already exists\")\n",
    "            \n",
    "        # Create bronze schema if it doesn't exist (for backup verification)\n",
    "        if 'bronze' not in existing_schemas:\n",
    "            print(\"\uD83D\uDD27 Creating bronze schema...\")\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.bronze\")\n",
    "            print(\"✅ Bronze schema created\")\n",
    "        else:\n",
    "            print(\"✅ Bronze schema already exists\")\n",
    "        \n",
    "        # Create COMPLETELY NEW TABLE with ALL STRING types to avoid ALL conflicts\n",
    "        new_stock_table = f\"{current_catalog}.silver.stock_data_consumer\"\n",
    "        \n",
    "        # Drop existing table if it exists\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {new_stock_table}\")\n",
    "            print(\"\uD83D\uDDD1️ Dropped any existing consumer table\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create silver stock_data table with ALL STRING types\n",
    "        stock_table_ddl = f\"\"\"\n",
    "        CREATE TABLE {new_stock_table} (\n",
    "            symbol STRING,\n",
    "            timestamp STRING,\n",
    "            open_price STRING,          -- STRING to avoid precision conflicts\n",
    "            high_price STRING,          -- STRING to avoid precision conflicts\n",
    "            low_price STRING,           -- STRING to avoid precision conflicts\n",
    "            close_price STRING,         -- STRING to avoid precision conflicts\n",
    "            volume STRING,              -- STRING to avoid type conflicts\n",
    "            source STRING,\n",
    "            volatility STRING,          -- STRING to avoid precision conflicts\n",
    "            price_range STRING,         -- STRING to avoid precision conflicts\n",
    "            volume_indicator STRING,\n",
    "            consumer_timestamp STRING,\n",
    "            partition_id STRING,\n",
    "            processing_mode STRING,\n",
    "            silver_processing_time STRING,\n",
    "            data_quality_score STRING, -- STRING to avoid precision conflicts\n",
    "            ingestion_time STRING,\n",
    "            processed_date STRING,\n",
    "            ingestion_batch STRING,\n",
    "            layer STRING,\n",
    "            ingestion_source STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(stock_table_ddl)\n",
    "        print(f\"✅ Table {new_stock_table} created with ALL STRING types\")\n",
    "        \n",
    "        # Verify tables exist\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {current_catalog}.silver\").collect()\n",
    "        table_names = [row.tableName for row in tables]\n",
    "        print(f\"\uD83D\uDCCB Available tables in silver schema: {table_names}\")\n",
    "        \n",
    "        return current_catalog, new_stock_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating schemas: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Create schemas and get catalog name\n",
    "current_catalog, stock_table_name = create_schemas_and_tables()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\uD83D\uDCC8 Running Stock Data Consumer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration with validation\n",
    "try:\n",
    "    eh_connection_string = dbutils.secrets.get(scope=\"stock-project\", key=\"event-hub-connection-string\")\n",
    "    if \"EntityPath=\" not in eh_connection_string:\n",
    "        eh_connection_string = f\"{eh_connection_string};EntityPath=stock-data-hub\"\n",
    "\n",
    "    storage_account_key = dbutils.secrets.get(scope=\"stock-project\", key=\"storage-account-key\")\n",
    "    storage_account_name = \"dlsstocksentiment2025\"\n",
    "    container_name = \"data\"\n",
    "\n",
    "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key)\n",
    "\n",
    "    adls_base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "    silver_stock_path = f\"{adls_base_path}/silver/stock_data_consumer\"\n",
    "\n",
    "    print(\"✅ Configuration loaded\")\n",
    "    \n",
    "except Exception as config_error:\n",
    "    print(f\"❌ Configuration error: {config_error}\")\n",
    "    raise\n",
    "\n",
    "def safe_save_to_catalog(df, table_name, mode=\"append\"):\n",
    "    \"\"\"Safely save to Unity Catalog with fallback\"\"\"\n",
    "    try:\n",
    "        (df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(mode)\n",
    "         .option(\"mergeSchema\", \"false\")  # No schema merging to avoid conflicts\n",
    "         .saveAsTable(table_name))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unity Catalog save failed for {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def safe_parse_json(message_body):\n",
    "    \"\"\"Safely parse JSON with better error handling\"\"\"\n",
    "    try:\n",
    "        # Handle empty or whitespace-only messages\n",
    "        if not message_body or not message_body.strip():\n",
    "            print(f\"⚠️ Empty message body\")\n",
    "            return None\n",
    "            \n",
    "        # Handle non-JSON messages (like \"Test connection message\")\n",
    "        message_body = message_body.strip()\n",
    "        if not message_body.startswith('{') and not message_body.startswith('['):\n",
    "            print(f\"⚠️ Non-JSON message: {message_body[:50]}...\")\n",
    "            return None\n",
    "            \n",
    "        # Try to parse JSON\n",
    "        data = json.loads(message_body)\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as je:\n",
    "        print(f\"⚠️ JSON decode error: {je} - Raw message: {message_body[:100]}...\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Message parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "def debug_print_message_structure(event, index):\n",
    "    \"\"\"Debug function to print message structure\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n\uD83D\uDD0D DEBUG - Message {index} structure:\")\n",
    "        print(f\"   Keys: {list(event.keys())}\")\n",
    "        print(f\"   data_type: {event.get('data_type', 'MISSING')}\")\n",
    "        print(f\"   symbol: {event.get('symbol', 'MISSING')}\")\n",
    "        \n",
    "        # Print all key-value pairs for first few messages\n",
    "        if index <= 2:\n",
    "            for key, value in event.items():\n",
    "                print(f\"   {key}: {value} (type: {type(value).__name__})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Debug print error: {e}\")\n",
    "\n",
    "def stock_only_consume_and_process():\n",
    "    \"\"\"Stock-only consumer with ALL STRING processing to avoid conflicts\"\"\"\n",
    "    \n",
    "    print(\"\\n\uD83D\uDD04 Starting stock data consumption and processing...\")\n",
    "    \n",
    "    # Step 1: Consume messages with timeout and validation\n",
    "    messages = []\n",
    "    stop_flag = threading.Event()\n",
    "    \n",
    "    def consumer_thread():\n",
    "        try:\n",
    "            client = EventHubConsumerClient.from_connection_string(\n",
    "                eh_connection_string,\n",
    "                consumer_group=\"$Default\",\n",
    "                transport_type=TransportType.AmqpOverWebsocket\n",
    "            )\n",
    "            \n",
    "            def message_handler(partition_context, event):\n",
    "                if stop_flag.is_set() or len(messages) >= 15:\n",
    "                    return\n",
    "                \n",
    "                if event and hasattr(event, 'body_as_str'):\n",
    "                    try:\n",
    "                        body = event.body_as_str(encoding='UTF-8')\n",
    "                        \n",
    "                        # Use safe JSON parsing\n",
    "                        data = safe_parse_json(body)\n",
    "                        if data is None:\n",
    "                            return  # Skip invalid messages\n",
    "                        \n",
    "                        # Filter for stock data only\n",
    "                        if data.get('data_type') == 'stock_price':\n",
    "                            data['consumer_timestamp'] = datetime.now(timezone.utc).isoformat()\n",
    "                            data['partition_id'] = partition_context.partition_id\n",
    "                            messages.append(data)\n",
    "                            print(f\"\uD83D\uDCC8 {len(messages)}: STOCK - {data.get('symbol', 'N/A')} @ ${data.get('close_price', 'N/A')}\")\n",
    "                            \n",
    "                            if len(messages) >= 15:\n",
    "                                stop_flag.set()\n",
    "                        else:\n",
    "                            # Skip non-stock messages silently\n",
    "                            pass\n",
    "                            \n",
    "                    except Exception as me:\n",
    "                        print(f\"⚠️ Message processing error: {me}\")\n",
    "            \n",
    "            with client:\n",
    "                client.receive(\n",
    "                    on_event=message_handler,\n",
    "                    starting_position=\"-1\",\n",
    "                    max_wait_time=10\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Consumer error: {e}\")\n",
    "            stop_flag.set()\n",
    "    \n",
    "    # Start consumer thread\n",
    "    thread = threading.Thread(target=consumer_thread, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    # Wait with extended timeout\n",
    "    start_time = time.time()\n",
    "    timeout_seconds = 20\n",
    "    while thread.is_alive() and time.time() - start_time < timeout_seconds and len(messages) < 15:\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    stop_flag.set()\n",
    "    thread.join(timeout=3)\n",
    "    \n",
    "    print(f\"✅ Consumed {len(messages)} stock messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"⚠️ No stock messages consumed - Event Hub might be empty or only contains news data\")\n",
    "        return\n",
    "    \n",
    "    # Debug: Print first message structure\n",
    "    if messages:\n",
    "        debug_print_message_structure(messages[0], 1)\n",
    "    \n",
    "    # Step 2: Process stock data with ALL STRING types\n",
    "    stock_events = [m for m in messages if m.get('data_type') == 'stock_price']\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCC8 Processing {len(stock_events)} stock events:\")\n",
    "    \n",
    "    # Process stocks with ALL STRING conversion to avoid any conflicts\n",
    "    processed_stocks = []\n",
    "    for i, event in enumerate(stock_events):\n",
    "        try:\n",
    "            print(f\"\\n\uD83D\uDD0D Processing stock {i+1}: {event.get('symbol', 'UNKNOWN')}\")\n",
    "            \n",
    "            # Debug: Show available fields in this event\n",
    "            print(f\"   Available fields: {list(event.keys())}\")\n",
    "            \n",
    "            # Safe field extraction with string conversion\n",
    "            symbol = str(event.get('symbol', f'UNKNOWN_{i}'))\n",
    "            timestamp = str(event.get('timestamp', datetime.now(timezone.utc).isoformat()))\n",
    "            \n",
    "            print(f\"   Symbol: {symbol}\")\n",
    "            print(f\"   Timestamp: {timestamp}\")\n",
    "            \n",
    "            # Safe conversion functions that return STRINGS\n",
    "            def safe_string_float(value, default=\"1.0\", field_name=\"unknown\"):\n",
    "                try:\n",
    "                    if value is None:\n",
    "                        print(f\"   {field_name} is None, using default {default}\")\n",
    "                        return str(default)\n",
    "                    result = str(float(value))\n",
    "                    print(f\"   {field_name}: {value} -> {result}\")\n",
    "                    return result\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"   {field_name} conversion error: {e}, using default {default}\")\n",
    "                    return str(default)\n",
    "            \n",
    "            def safe_string_int(value, default=\"0\", field_name=\"unknown\"):\n",
    "                try:\n",
    "                    if value is None:\n",
    "                        print(f\"   {field_name} is None, using default {default}\")\n",
    "                        return str(default)\n",
    "                    result = str(int(float(value)))\n",
    "                    print(f\"   {field_name}: {value} -> {result}\")\n",
    "                    return result\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"   {field_name} conversion error: {e}, using default {default}\")\n",
    "                    return str(default)\n",
    "            \n",
    "            # Extract price fields with fallbacks - ALL AS STRINGS\n",
    "            close_price_str = safe_string_float(event.get('close_price', event.get('price', 1.0)), \"1.0\", 'close_price')\n",
    "            open_price_str = safe_string_float(event.get('open_price', event.get('close_price', 1.0)), close_price_str, 'open_price')\n",
    "            high_price_str = safe_string_float(event.get('high_price', event.get('close_price', 1.0)), close_price_str, 'high_price')\n",
    "            low_price_str = safe_string_float(event.get('low_price', event.get('close_price', 1.0)), close_price_str, 'low_price')\n",
    "            volume_str = safe_string_int(event.get('volume', 0), \"0\", 'volume')\n",
    "            \n",
    "            # Convert to float for calculations, then back to string\n",
    "            try:\n",
    "                close_price_float = float(close_price_str)\n",
    "                high_price_float = float(high_price_str)\n",
    "                low_price_float = float(low_price_str)\n",
    "                \n",
    "                # Calculate enhanced metrics\n",
    "                price_range = high_price_float - low_price_float\n",
    "                volatility = price_range / close_price_float if close_price_float > 0 else 0.0\n",
    "                \n",
    "                # Volume indicator\n",
    "                volume_int = int(float(volume_str))\n",
    "                if volume_int > 50000000:\n",
    "                    volume_indicator = 'high'\n",
    "                elif volume_int > 20000000:\n",
    "                    volume_indicator = 'medium'\n",
    "                else:\n",
    "                    volume_indicator = 'low'\n",
    "                \n",
    "                print(f\"   Final prices - O:{open_price_str}, H:{high_price_str}, L:{low_price_str}, C:{close_price_str}, V:{volume_str}\")\n",
    "                \n",
    "                # ALL VALUES AS STRINGS to eliminate any type conflicts\n",
    "                processed_stock = {\n",
    "                    'symbol': str(symbol),\n",
    "                    'timestamp': str(timestamp),\n",
    "                    'open_price': str(open_price_str),\n",
    "                    'high_price': str(high_price_str),\n",
    "                    'low_price': str(low_price_str),\n",
    "                    'close_price': str(close_price_str),\n",
    "                    'volume': str(volume_str),\n",
    "                    'source': str(event.get('source', 'event_hub')),\n",
    "                    'volatility': str(python_round(volatility, 6)),\n",
    "                    'price_range': str(python_round(price_range, 4)),\n",
    "                    'volume_indicator': str(volume_indicator),\n",
    "                    'consumer_timestamp': str(event.get('consumer_timestamp', '')),\n",
    "                    'partition_id': str(event.get('partition_id', '')),\n",
    "                    'processing_mode': str('streaming_stock_consumer'),\n",
    "                    'silver_processing_time': str(datetime.now(timezone.utc).isoformat()),\n",
    "                    'data_quality_score': str(1.0)\n",
    "                }\n",
    "                \n",
    "                processed_stocks.append(processed_stock)\n",
    "                print(f\"✅ Stock {i+1}: {symbol} @ ${close_price_str} processed successfully\")\n",
    "                \n",
    "            except Exception as calc_error:\n",
    "                print(f\"❌ Calculation error for {symbol}: {calc_error}\")\n",
    "                continue\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Stock processing error {i+1}: {e}\")\n",
    "            print(f\"   Full error details:\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Step 3: Save stock data with ALL STRING metadata\n",
    "    batch_id = f\"stock_consumer_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    save_success = False\n",
    "    \n",
    "    if processed_stocks:\n",
    "        try:\n",
    "            print(f\"\\n\uD83D\uDCBE Saving {len(processed_stocks)} processed stocks...\")\n",
    "            \n",
    "            # Add metadata as strings\n",
    "            current_time_str = datetime.now(timezone.utc).isoformat()\n",
    "            current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            for record in processed_stocks:\n",
    "                record[\"ingestion_time\"] = str(current_time_str)\n",
    "                record[\"processed_date\"] = str(current_date_str)\n",
    "                record[\"ingestion_batch\"] = str(batch_id)\n",
    "                record[\"layer\"] = str(\"silver\")\n",
    "                record[\"ingestion_source\"] = str(\"stock_consumer\")\n",
    "            \n",
    "            stock_df = spark.createDataFrame(processed_stocks)\n",
    "            \n",
    "            # Save to ADLS (primary) - NO SCHEMA MERGING\n",
    "            (stock_df.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"append\")\n",
    "             .option(\"mergeSchema\", \"false\")\n",
    "             .save(silver_stock_path))\n",
    "            \n",
    "            print(\"✅ Saved stocks to Silver layer (ADLS)\")\n",
    "            save_success = True\n",
    "            \n",
    "            # Try Unity Catalog (secondary)\n",
    "            if current_catalog and stock_table_name and safe_save_to_catalog(stock_df, stock_table_name):\n",
    "                print(\"✅ Also saved stocks to Unity Catalog\")\n",
    "            else:\n",
    "                print(\"⚠️ Unity Catalog stock save skipped\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Stock save error: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Step 4: Verification and reporting\n",
    "    print(f\"\\n\uD83D\uDD0D Verification:\")\n",
    "    try:\n",
    "        if save_success:\n",
    "            try:\n",
    "                # Verify ADLS storage\n",
    "                stock_files = dbutils.fs.ls(silver_stock_path)\n",
    "                print(f\"✅ Silver layer stock files: {len(stock_files)}\")\n",
    "                \n",
    "                # Verify data integrity\n",
    "                stock_verify = spark.read.format(\"delta\").load(silver_stock_path)\n",
    "                total_stocks = stock_verify.count()\n",
    "                recent_stocks = stock_verify.filter(col(\"ingestion_source\") == \"stock_consumer\").count()\n",
    "                print(f\"✅ Verified: {recent_stocks} new stock records ({total_stocks} total)\")\n",
    "                \n",
    "                # Show sample of what was saved\n",
    "                print(f\"\\n\uD83D\uDCCA Sample Stock Records:\")\n",
    "                (stock_verify\n",
    "                 .filter(col(\"ingestion_source\") == \"stock_consumer\")\n",
    "                 .select(\"symbol\", \"close_price\", \"volume\", \"volatility\", \"volume_indicator\", \"silver_processing_time\")\n",
    "                 .orderBy(col(\"silver_processing_time\").desc())\n",
    "                 .limit(5)\n",
    "                 .show(truncate=False))\n",
    "                \n",
    "                # Show summary statistics\n",
    "                print(f\"\\n\uD83D\uDCC8 Stock Data Summary:\")\n",
    "                (stock_verify\n",
    "                 .filter(col(\"ingestion_source\") == \"stock_consumer\")\n",
    "                 .groupBy(\"symbol\")\n",
    "                 .agg(\n",
    "                     count(\"*\").alias(\"record_count\")\n",
    "                 )\n",
    "                 .orderBy(\"symbol\")\n",
    "                 .show(truncate=False))\n",
    "                \n",
    "            except Exception as ve:\n",
    "                print(f\"⚠️ Stock verification error: {ve}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Verification error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\uD83D\uDCCB Final Summary:\")\n",
    "    print(f\"✅ Consumed: {len(messages)} stock messages\")\n",
    "    print(f\"✅ Processed: {len(processed_stocks)} stock records\")\n",
    "    print(f\"✅ ADLS save: {save_success}\")\n",
    "    print(f\"\uD83D\uDCC1 Batch ID: {batch_id}\")\n",
    "    print(f\"\uD83C\uDFAF Focus: Stock data with ALL STRING types (no conflicts possible)\")\n",
    "    print(f\"\uD83D\uDCCB New Table: {stock_table_name}\")\n",
    "\n",
    "# Execute consumer with top-level error handling\n",
    "try:\n",
    "    print(\"\uD83D\uDE80 Starting Stock Consumer Script\")\n",
    "    print(f\"⏰ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    stock_only_consume_and_process()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Stock consumer failed: {e}\")\n",
    "    print(\"\uD83D\uDCCB Full error traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n⏰ COMPLETED: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"\uD83C\uDFAF Stock data consumer with ALL STRING types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6642ab8d-3dfd-4326-aa76-cf369e18b78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Airflow Integration - Success/Failure Reporting\n",
    "\n",
    "try:\n",
    "    # If we reach here, notebook executed successfully\n",
    "    success_result = {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"message\": \"Notebook execution completed successfully\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"records_processed\": locals().get('total_records_processed', 0),  # Update based on your variables\n",
    "        \"data_quality_score\": locals().get('data_quality_score', 1.0)     # Update based on your variables\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Notebook Success:\")\n",
    "    print(json.dumps(success_result, indent=2))\n",
    "    \n",
    "    # Exit with success status for Airflow\n",
    "    dbutils.notebook.exit(success_result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # If any error occurs, report failure\n",
    "    failure_result = {\n",
    "        \"status\": \"FAILED\", \n",
    "        \"message\": f\"Notebook execution failed: {str(e)}\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"error_type\": type(e).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"❌ Notebook Failure:\")\n",
    "    print(json.dumps(failure_result, indent=2))\n",
    "    \n",
    "    # Exit with failure status for Airflow\n",
    "    dbutils.notebook.exit(failure_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Event_Hub_Stock_Consumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}