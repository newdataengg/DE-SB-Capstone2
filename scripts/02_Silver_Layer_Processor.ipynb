{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868d9cab-1947-4729-8886-62380a330694",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airflow Integration Parameters\n",
    "\n",
    "try:\n",
    "    # Create widgets for Airflow parameters\n",
    "    dbutils.widgets.text(\"batch_id\", \"manual_run\", \"Batch ID from Airflow\")\n",
    "    dbutils.widgets.text(\"execution_date\", \"\", \"Execution Date from Airflow\") \n",
    "    dbutils.widgets.text(\"force_refresh\", \"false\", \"Force data refresh\")\n",
    "    dbutils.widgets.text(\"quality_threshold\", \"0.8\", \"Data quality threshold\")\n",
    "    dbutils.widgets.text(\"dag_run_id\", \"\", \"DAG Run ID\")\n",
    "    \n",
    "    # Get parameter values\n",
    "    batch_id = dbutils.widgets.get(\"batch_id\")\n",
    "    execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "    force_refresh = dbutils.widgets.get(\"force_refresh\").lower() == \"true\"\n",
    "    quality_threshold = float(dbutils.widgets.get(\"quality_threshold\"))\n",
    "    dag_run_id = dbutils.widgets.get(\"dag_run_id\")\n",
    "    \n",
    "    print(f\"\uD83C\uDFAF Airflow Parameters:\")\n",
    "    print(f\"   Batch ID: {batch_id}\")\n",
    "    print(f\"   Execution Date: {execution_date}\")\n",
    "    print(f\"   Force Refresh: {force_refresh}\")\n",
    "    print(f\"   Quality Threshold: {quality_threshold}\")\n",
    "    print(f\"   DAG Run ID: {dag_run_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Widget creation failed (normal in some contexts): {e}\")\n",
    "    # Fallback values for manual runs\n",
    "    batch_id = \"manual_run\"\n",
    "    execution_date = \"\"\n",
    "    force_refresh = False\n",
    "    quality_threshold = 0.8\n",
    "    dag_run_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77d68f5-afda-40ed-b7a1-76560f3abc54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83E\uDD48 Silver Layer Processor - Complete Combined Version\n============================================================\n⏰ Started: 2025-07-21 22:23:12\n\uD83C\uDFAF Focus: Bronze → Silver with explicit schemas and comprehensive analytics\n\uD83D\uDCCB Current catalog: databricks_stock_sentiment_canada\n✅ Configuration loaded successfully\n\uD83D\uDCCA Bronze Stock Table: databricks_stock_sentiment_canada.bronze.historical_stock_data\n\uD83D\uDCF0 Bronze News Table: databricks_stock_sentiment_canada.bronze.historical_news_data\n\uD83D\uDCCA Enhanced processing configuration:\n   batch_size: 1000\n   finbert_confidence_threshold: 0.6\n   data_quality_threshold: 0.7\n   technical_indicators_window: 20\n   sentiment_aggregation_window: 7\n   correlation_analysis_enabled: True\n   feature_engineering_enabled: True\n   max_news_records_per_batch: 50\n   enable_explicit_schema: True\n✅ Explicit schemas defined for type-safe DataFrame creation\n\uD83D\uDCCA Stock schema: 51 fields\n\uD83D\uDCF0 News schema: 43 fields\n\uD83E\uDD16 Loading enhanced FinBERT model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5699bde4743647aba2f3421b75e7a03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed35c64393e453082c5489ec3646e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135b2052da0b4b2fa4c2bf1b4ab57b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4278a508e5b3443fb0d44fa1d93c9015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eec69c192b64e34a21db656dd5558d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3504d19467244a93a98669ec3f4167ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FinBERT model ready for production\n\uD83E\uDD16 FinBERT Status: Production Ready\n\uD83C\uDFD7️ Creating enhanced Silver layer tables...\n✅ Silver schema created in catalog: databricks_stock_sentiment_canada\n✅ Created enhanced stock table: databricks_stock_sentiment_canada.silver.enhanced_stock_data\n✅ Created enhanced news table: databricks_stock_sentiment_canada.silver.enhanced_news_data\n✅ Enhanced Silver layer tables created successfully\n\uD83D\uDE80 Launching Silver Layer Processing\n\uD83C\uDFAF Key Features:\n   • Explicit schemas for type safety\n   • Production FinBERT sentiment analysis\n   • Comprehensive technical indicators\n   • Enhanced error handling and stability\n\uD83E\uDDF9 Cleaning up existing Silver tables for fresh schema...\n✅ Dropped existing table: databricks_stock_sentiment_canada.silver.enhanced_stock_data\n✅ Dropped existing table: databricks_stock_sentiment_canada.silver.enhanced_news_data\n✅ Silver tables cleanup completed\n\n\uD83E\uDD48 Starting Combined Silver Layer Processing\n\uD83D\uDCCB Batch ID: silver_combined_20250721_222342\n⏰ Started: 2025-07-21 22:23:42\n\uD83C\uDFAF Features: Explicit schemas + FinBERT + Technical indicators\n\n\uD83D\uDCCA Phase 1: Enhanced Stock Processing\n\uD83D\uDCC8 Processing 6 stock records\n\uD83D\uDCCA Calculating comprehensive technical indicators...\n✅ Comprehensive technical indicators calculated with all required fields\n✅ Created 6 enhanced stock records\n\n\uD83E\uDD16 Phase 2: Enhanced News Processing with FinBERT\n\uD83D\uDCF0 Found 30 news records\n\uD83D\uDD04 Processing 30 high-quality news records with FinBERT...\n\uD83D\uDD04 Processed 10/30 (33.3%)\n\uD83D\uDD04 Processed 20/30 (66.7%)\n\uD83D\uDD04 Processed 30/30 (100.0%)\n\uD83D\uDCBE Creating DataFrame with explicit schema for 30 records...\n✅ Created 30 enhanced news records\n\n⚡ Phase 3: Table Optimization\n✅ Optimized databricks_stock_sentiment_canada.silver.enhanced_stock_data (6 records)\n✅ Optimized databricks_stock_sentiment_canada.silver.enhanced_news_data (30 records)\n\n================================================================================\n\uD83E\uDD48 SILVER LAYER PROCESSING SUMMARY\n================================================================================\n\uD83D\uDCCB Processing Details:\n   Batch ID: silver_combined_20250721_222342\n   Duration: 0.30 minutes\n   Start: 22:23:42\n   End: 22:24:00\n\n\uD83D\uDCCA Processing Results:\n   \uD83D\uDCC8 Stock Records: 6 → 6\n   \uD83D\uDCF0 News Records: 30 → 30\n   \uD83C\uDFAF Total Enhanced: 36 records\n\n\uD83E\uDD16 FinBERT Processing:\n   Model Status: Production\n   Total Calls: 30\n   Avg Time: 150.55ms\n\n\uD83C\uDFAF Enhanced Features Created:\n   ✅ Technical indicators (RSI, MACD, Bollinger Bands)\n   ✅ FinBERT sentiment analysis with confidence\n   ✅ Type-safe DataFrame creation with explicit schemas\n   ✅ Quality scoring and reliability metrics\n   ✅ Optimized for correlation analysis\n\n✅ STATUS: SUCCESS - Ready for Gold layer correlation analysis!\n================================================================================\n\n\uD83C\uDF89 Combined Silver Layer Processing Complete!\n\n\uD83D\uDCCA Final Silver Layer Status:\n   \uD83D\uDCC8 Enhanced Stock Records: 6\n   \uD83D\uDCF0 Enhanced News Records: 30\n\n\uD83D\uDD17 Correlation Analysis Ready:\n   ✅ Stock technical indicators available\n   ✅ News sentiment analysis complete\n   ✅ Both datasets in Silver layer\n   \uD83D\uDE80 Ready for Gold layer processing!\n\n\uD83D\uDCCA Stock Data Preview:\n+------+----------+-----------+-------+----------------+\n|symbol|      date|close_price|rsi_14d|technical_signal|\n+------+----------+-----------+-------+----------------+\n|  AAPL|2025-07-18|     211.18|   50.0|            hold|\n|  AMZN|2025-07-18|     226.13|   50.0|            hold|\n| GOOGL|2025-07-18|     185.06|   50.0|            hold|\n+------+----------+-----------+-------+----------------+\n\n\n\uD83D\uDCF0 News Data Preview:\n+------------------------------------------------------------------------------------------+-------------+-------------+------------------+\n|title                                                                                     |finbert_label|finbert_score|finbert_confidence|\n+------------------------------------------------------------------------------------------+-------------+-------------+------------------+\n|Mamadou Sarr set to rejoin RC Strasbourg on loan from Chelsea — report                    |positive     |0.8917       |0.8917            |\n|Sad Double Heartbreak for 'Love Actually' Star Martine McCutcheon After Split From Husband|neutral      |0.3008       |0.596             |\n|The Westpac share price is a buy – UBS                                                    |positive     |0.8142       |0.8142            |\n+------------------------------------------------------------------------------------------+-------------+-------------+------------------+\n\n\n\uD83D\uDD0D Final Silver Layer Verification\n============================================================\n\n\uD83D\uDCCA Stock Data Verification:\n   \uD83D\uDCC8 Records: 6\n   \uD83D\uDCCA Technical Indicators Quality: 100.00%\n   ✅ Stock data ready for correlation analysis\n\n\uD83D\uDCF0 News Data Verification:\n   \uD83D\uDCF0 Records: 30\n   \uD83E\uDD16 High Confidence Predictions: 83.33%\n   \uD83D\uDCCA Average FinBERT Confidence: 0.776\n   \uD83C\uDFAF Sentiment Variety: 3 labels\n   ✅ News data ready for correlation analysis\n\n\uD83D\uDD17 Correlation Analysis Readiness:\n   ✅ Both datasets are ready for correlation analysis\n   ✅ Technical indicators calculated and validated\n   ✅ FinBERT sentiment analysis completed\n   ✅ Data quality thresholds met\n\n\uD83D\uDCCA Overall Silver Layer Quality Score: 90.0%\n\n\uD83C\uDF8A SILVER LAYER PROCESSOR COMPLETED\n================================================================================\n\uD83C\uDFAF STATUS: READY FOR ADVANCED ANALYTICS\n\n⏰ Processing completed: 2025-07-21 22:24:03\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC **Enhanced Analytics with Explicit Schema and FinBERT Sentiment Analysis**\n",
    "# MAGIC \n",
    "# MAGIC This notebook combines the best features from both approaches:\n",
    "# MAGIC - Explicit schema definitions for type safety and DataFrame creation stability\n",
    "# MAGIC - Complete pipeline with comprehensive technical indicators\n",
    "# MAGIC - Production-grade FinBERT sentiment analysis with fallback mechanisms\n",
    "# MAGIC - Advanced quality scoring and correlation readiness\n",
    "# MAGIC - Full verification and optimization suite\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 1. Setup and Configuration with Enhanced Error Handling\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Store Python's built-in functions before PySpark imports override them\n",
    "import builtins\n",
    "python_round = builtins.round\n",
    "python_min = builtins.min\n",
    "python_max = builtins.max\n",
    "python_abs = builtins.abs\n",
    "\n",
    "# Set environment variables BEFORE any torch imports\n",
    "import os\n",
    "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "os.environ['MASTER_PORT'] = '12355' \n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import traceback\n",
    "\n",
    "# Import torch with distributed training disabled\n",
    "import torch\n",
    "torch.distributed.is_available = lambda: False\n",
    "torch.distributed.is_initialized = lambda: False\n",
    "\n",
    "print(\"\uD83E\uDD48 Silver Layer Processor - Complete Combined Version\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"⏰ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\uD83C\uDFAF Focus: Bronze → Silver with explicit schemas and comprehensive analytics\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Enhanced Configuration with catalog awareness\n",
    "try:\n",
    "    # Get current catalog for table references\n",
    "    current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "    print(f\"\uD83D\uDCCB Current catalog: {current_catalog}\")\n",
    "    \n",
    "    # Table references with catalog context\n",
    "    bronze_stock_table = f\"{current_catalog}.bronze.historical_stock_data\"\n",
    "    bronze_news_table = f\"{current_catalog}.bronze.historical_news_data\"\n",
    "    silver_stock_table = f\"{current_catalog}.silver.enhanced_stock_data\"\n",
    "    silver_news_table = f\"{current_catalog}.silver.enhanced_news_data\"\n",
    "    silver_metrics_table = f\"{current_catalog}.silver.processing_metrics\"\n",
    "    \n",
    "    print(\"✅ Configuration loaded successfully\")\n",
    "    print(f\"\uD83D\uDCCA Bronze Stock Table: {bronze_stock_table}\")\n",
    "    print(f\"\uD83D\uDCF0 Bronze News Table: {bronze_news_table}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Configuration error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Enhanced processing configuration\n",
    "SILVER_CONFIG = {\n",
    "    \"batch_size\": 1000,\n",
    "    \"finbert_confidence_threshold\": 0.6,\n",
    "    \"data_quality_threshold\": 0.7,\n",
    "    \"technical_indicators_window\": 20,\n",
    "    \"sentiment_aggregation_window\": 7,\n",
    "    \"correlation_analysis_enabled\": True,\n",
    "    \"feature_engineering_enabled\": True,\n",
    "    \"max_news_records_per_batch\": 50,  # Limit for stable processing\n",
    "    \"enable_explicit_schema\": True\n",
    "}\n",
    "\n",
    "print(f\"\uD83D\uDCCA Enhanced processing configuration:\")\n",
    "for key, value in SILVER_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 2. Explicit Schema Definitions for Type Safety\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_enhanced_stock_schema():\n",
    "    \"\"\"Define comprehensive schema for enhanced stock data\"\"\"\n",
    "    return StructType([\n",
    "        # Basic stock data\n",
    "        StructField(\"symbol\", StringType(), True),\n",
    "        StructField(\"date\", DateType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"open_price\", DoubleType(), True),\n",
    "        StructField(\"high_price\", DoubleType(), True),\n",
    "        StructField(\"low_price\", DoubleType(), True),\n",
    "        StructField(\"close_price\", DoubleType(), True),\n",
    "        StructField(\"adjusted_close\", DoubleType(), True),\n",
    "        StructField(\"volume\", LongType(), True),\n",
    "        StructField(\"split_coefficient\", DoubleType(), True),\n",
    "        StructField(\"dividend_amount\", DoubleType(), True),\n",
    "        \n",
    "        # Price movement indicators\n",
    "        StructField(\"price_change\", DoubleType(), True),\n",
    "        StructField(\"price_change_pct\", DoubleType(), True),\n",
    "        StructField(\"daily_range\", DoubleType(), True),\n",
    "        StructField(\"daily_range_pct\", DoubleType(), True),\n",
    "        StructField(\"gap_pct\", DoubleType(), True),\n",
    "        \n",
    "        # Volume indicators\n",
    "        StructField(\"volume_change\", LongType(), True),\n",
    "        StructField(\"volume_change_pct\", DoubleType(), True),\n",
    "        StructField(\"volume_ma_5d\", DoubleType(), True),\n",
    "        StructField(\"volume_ma_20d\", DoubleType(), True),\n",
    "        StructField(\"volume_ratio\", DoubleType(), True),\n",
    "        \n",
    "        # Moving averages\n",
    "        StructField(\"sma_5d\", DoubleType(), True),\n",
    "        StructField(\"sma_10d\", DoubleType(), True),\n",
    "        StructField(\"sma_20d\", DoubleType(), True),\n",
    "        StructField(\"sma_50d\", DoubleType(), True),\n",
    "        StructField(\"ema_12d\", DoubleType(), True),\n",
    "        StructField(\"ema_26d\", DoubleType(), True),\n",
    "        \n",
    "        # Technical indicators\n",
    "        StructField(\"rsi_14d\", DoubleType(), True),\n",
    "        StructField(\"macd\", DoubleType(), True),\n",
    "        StructField(\"macd_signal\", DoubleType(), True),\n",
    "        StructField(\"macd_histogram\", DoubleType(), True),\n",
    "        StructField(\"bollinger_upper\", DoubleType(), True),\n",
    "        StructField(\"bollinger_lower\", DoubleType(), True),\n",
    "        StructField(\"bollinger_position\", DoubleType(), True),\n",
    "        \n",
    "        # Volatility measures\n",
    "        StructField(\"volatility_5d\", DoubleType(), True),\n",
    "        StructField(\"volatility_20d\", DoubleType(), True),\n",
    "        StructField(\"atr_14d\", DoubleType(), True),\n",
    "        \n",
    "        # Trend analysis\n",
    "        StructField(\"trend_direction\", StringType(), True),\n",
    "        StructField(\"trend_strength\", DoubleType(), True),\n",
    "        StructField(\"support_level\", DoubleType(), True),\n",
    "        StructField(\"resistance_level\", DoubleType(), True),\n",
    "        \n",
    "        # Signal generation\n",
    "        StructField(\"technical_signal\", StringType(), True),\n",
    "        StructField(\"market_sentiment\", StringType(), True),\n",
    "        StructField(\"momentum_indicator\", StringType(), True),\n",
    "        \n",
    "        # Quality metrics\n",
    "        StructField(\"data_quality_score\", DoubleType(), True),\n",
    "        StructField(\"completeness_score\", DoubleType(), True),\n",
    "        StructField(\"reliability_score\", DoubleType(), True),\n",
    "        \n",
    "        # Processing metadata\n",
    "        StructField(\"source\", StringType(), True),\n",
    "        StructField(\"processing_timestamp\", TimestampType(), True),\n",
    "        StructField(\"ingestion_batch\", StringType(), True),\n",
    "        StructField(\"processed_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "def get_enhanced_news_schema():\n",
    "    \"\"\"Define comprehensive schema for enhanced news data with FinBERT\"\"\"\n",
    "    return StructType([\n",
    "        # Basic news data\n",
    "        StructField(\"article_id\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"content\", StringType(), True),\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"source\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), True),\n",
    "        StructField(\"published_at\", TimestampType(), True),\n",
    "        StructField(\"published_date\", DateType(), True),\n",
    "        \n",
    "        # Content metrics\n",
    "        StructField(\"content_length\", IntegerType(), True),\n",
    "        StructField(\"title_length\", IntegerType(), True),\n",
    "        StructField(\"readability_score\", DoubleType(), True),\n",
    "        StructField(\"financial_relevance_score\", DoubleType(), True),\n",
    "        \n",
    "        # Entity extraction\n",
    "        StructField(\"mentioned_symbols\", ArrayType(StringType()), True),\n",
    "        StructField(\"financial_entities\", ArrayType(StringType()), True),\n",
    "        StructField(\"key_financial_terms\", ArrayType(StringType()), True),\n",
    "        StructField(\"named_entities\", ArrayType(StringType()), True),\n",
    "        \n",
    "        # Original sentiment indicators\n",
    "        StructField(\"original_sentiment_indicators\", MapType(StringType(), DoubleType()), True),\n",
    "        \n",
    "        # FinBERT sentiment analysis\n",
    "        StructField(\"finbert_label\", StringType(), True),\n",
    "        StructField(\"finbert_score\", DoubleType(), True),\n",
    "        StructField(\"finbert_confidence\", DoubleType(), True),\n",
    "        StructField(\"finbert_negative\", DoubleType(), True),\n",
    "        StructField(\"finbert_neutral\", DoubleType(), True),\n",
    "        StructField(\"finbert_positive\", DoubleType(), True),\n",
    "        StructField(\"finbert_processing_method\", StringType(), True),\n",
    "        StructField(\"finbert_processing_time_ms\", DoubleType(), True),\n",
    "        \n",
    "        # Enhanced sentiment features\n",
    "        StructField(\"sentiment_intensity\", StringType(), True),\n",
    "        StructField(\"sentiment_subjectivity\", DoubleType(), True),\n",
    "        StructField(\"emotional_tone\", StringType(), True),\n",
    "        StructField(\"urgency_score\", DoubleType(), True),\n",
    "        \n",
    "        # Categorization\n",
    "        StructField(\"news_category\", StringType(), True),\n",
    "        StructField(\"market_impact_category\", StringType(), True),\n",
    "        StructField(\"time_sensitivity\", StringType(), True),\n",
    "        \n",
    "        # Quality scoring\n",
    "        StructField(\"content_quality_score\", DoubleType(), True),\n",
    "        StructField(\"sentiment_quality_score\", DoubleType(), True),\n",
    "        StructField(\"overall_reliability_score\", DoubleType(), True),\n",
    "        \n",
    "        # Temporal features\n",
    "        StructField(\"market_hours_flag\", BooleanType(), True),\n",
    "        StructField(\"weekday_flag\", BooleanType(), True),\n",
    "        StructField(\"earnings_season_flag\", BooleanType(), True),\n",
    "        \n",
    "        # Processing metadata\n",
    "        StructField(\"processing_timestamp\", TimestampType(), True),\n",
    "        StructField(\"data_source\", StringType(), True),\n",
    "        StructField(\"ingestion_batch\", StringType(), True),\n",
    "        StructField(\"processed_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "print(\"✅ Explicit schemas defined for type-safe DataFrame creation\")\n",
    "print(f\"\uD83D\uDCCA Stock schema: {len(get_enhanced_stock_schema())} fields\")\n",
    "print(f\"\uD83D\uDCF0 News schema: {len(get_enhanced_news_schema())} fields\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 3. Enhanced FinBERT\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load FinBERT with comprehensive error handling\n",
    "finbert_ready = False\n",
    "processing_stats = {\"finbert_calls\": 0, \"total_time\": 0.0, \"avg_time_per_call\": 0.0}\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\uD83E\uDD16 Loading enhanced FinBERT model...\")\n",
    "    \n",
    "    # Force CPU-only mode\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # Load tokenizer with explicit settings\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"ProsusAI/finbert\",\n",
    "        use_fast=True,\n",
    "        local_files_only=False,\n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    \n",
    "    # Load model with CPU-only configuration\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"ProsusAI/finbert\",\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=None,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=False\n",
    "    )\n",
    "    \n",
    "    # Move to CPU and disable gradients\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    finbert_ready = True\n",
    "    print(\"✅ FinBERT model ready for production\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ FinBERT loading failed, using enhanced fallback: {e}\")\n",
    "    finbert_ready = False\n",
    "\n",
    "def enhanced_financial_preprocessing(text):\n",
    "    \"\"\"Enhanced preprocessing for financial text\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"No content available\"\n",
    "    \n",
    "    # Handle complex financial sentences\n",
    "    text = re.sub(r',(\\\\s+(?:but|although|while|however))', r' \\\\1', text)\n",
    "    \n",
    "    # Focus on sentiment-bearing parts after conjunctions\n",
    "    sentiment_markers = ['but', 'however', 'although', 'despite', 'yet', 'while', 'nevertheless']\n",
    "    for marker in sentiment_markers:\n",
    "        if marker in text.lower():\n",
    "            parts = text.lower().split(marker)\n",
    "            if len(parts) > 1:\n",
    "                text = parts[-1].strip()\n",
    "                break\n",
    "    \n",
    "    # Clean financial patterns\n",
    "    text = re.sub(r'\\\\$\\\\s*\\\\d+(?:\\\\.\\\\d+)?[BMK]?', '[AMOUNT]', text)\n",
    "    text = re.sub(r'\\\\d+\\\\.\\\\d+%', '[PERCENT]', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text[:512]\n",
    "\n",
    "def apply_production_finbert_sentiment(text):\n",
    "    \"\"\"Production-grade FinBERT sentiment analysis with comprehensive fallback\"\"\"\n",
    "    \n",
    "    global processing_stats\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if not finbert_ready:\n",
    "        # Enhanced fallback sentiment analysis\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        sentiment_weights = {\n",
    "            'strong_positive': {\n",
    "                'words': ['surge', 'soars', 'beats', 'exceeds', 'outperforms', 'bullish', 'rally', 'breakthrough', 'skyrockets'],\n",
    "                'weight': 0.4\n",
    "            },\n",
    "            'positive': {\n",
    "                'words': ['gains', 'rises', 'up', 'growth', 'strong', 'positive', 'improved', 'boost', 'optimistic', 'upbeat'],\n",
    "                'weight': 0.2\n",
    "            },\n",
    "            'strong_negative': {\n",
    "                'words': ['plummet', 'crashes', 'tanks', 'collapses', 'bearish', 'panic', 'crisis', 'plunges', 'devastated'],\n",
    "                'weight': -0.4\n",
    "            },\n",
    "            'negative': {\n",
    "                'words': ['falls', 'drops', 'declines', 'weak', 'negative', 'pressure', 'concerns', 'pessimistic', 'worried'],\n",
    "                'weight': -0.2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        total_score = 0.0\n",
    "        word_count = 0\n",
    "        \n",
    "        for category, data in sentiment_weights.items():\n",
    "            matches = sum(1 for word in data['words'] if word in text_lower)\n",
    "            total_score += matches * data['weight']\n",
    "            word_count += matches\n",
    "        \n",
    "        sentiment_score = python_max(-1.0, python_min(1.0, total_score)) if word_count > 0 else 0.0\n",
    "        \n",
    "        if sentiment_score > 0.2:\n",
    "            label = 'positive'\n",
    "            confidence = python_min(0.9, 0.6 + python_abs(sentiment_score))\n",
    "        elif sentiment_score < -0.2:\n",
    "            label = 'negative' \n",
    "            confidence = python_min(0.9, 0.6 + python_abs(sentiment_score))\n",
    "        else:\n",
    "            label = 'neutral'\n",
    "            confidence = 0.7\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        processing_stats[\"finbert_calls\"] += 1\n",
    "        processing_stats[\"total_time\"] += processing_time\n",
    "        \n",
    "        return {\n",
    "            'finbert_label': label,\n",
    "            'finbert_score': sentiment_score,\n",
    "            'finbert_confidence': confidence,\n",
    "            'finbert_negative': 0.5 - sentiment_score/2 if sentiment_score <= 0 else 0.1,\n",
    "            'finbert_neutral': 0.5 - python_abs(sentiment_score)/2,\n",
    "            'finbert_positive': 0.5 + sentiment_score/2 if sentiment_score >= 0 else 0.1,\n",
    "            'processing_method': 'enhanced_fallback',\n",
    "            'processing_time_ms': processing_time * 1000\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        clean_text = enhanced_financial_preprocessing(text)\n",
    "        \n",
    "        if not clean_text or clean_text == \"No content available\":\n",
    "            processing_time = time.time() - start_time\n",
    "            processing_stats[\"finbert_calls\"] += 1\n",
    "            processing_stats[\"total_time\"] += processing_time\n",
    "            \n",
    "            return {\n",
    "                'finbert_label': 'neutral',\n",
    "                'finbert_score': 0.0,\n",
    "                'finbert_confidence': 0.5,\n",
    "                'finbert_negative': 0.33,\n",
    "                'finbert_neutral': 0.34,\n",
    "                'finbert_positive': 0.33,\n",
    "                'processing_method': 'empty_text',\n",
    "                'processing_time_ms': processing_time * 1000\n",
    "            }\n",
    "        \n",
    "        # CPU-only tokenization\n",
    "        inputs = tokenizer(\n",
    "            clean_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        predictions = predictions.cpu().numpy()[0]\n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        \n",
    "        max_idx = np.argmax(predictions)\n",
    "        predicted_label = labels[max_idx]\n",
    "        confidence = float(predictions[max_idx])\n",
    "        \n",
    "        if predicted_label == 'positive':\n",
    "            score = confidence\n",
    "        elif predicted_label == 'negative':\n",
    "            score = -confidence\n",
    "        else:\n",
    "            score = float(predictions[2]) - float(predictions[0])\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        processing_stats[\"finbert_calls\"] += 1\n",
    "        processing_stats[\"total_time\"] += processing_time\n",
    "        processing_stats[\"avg_time_per_call\"] = processing_stats[\"total_time\"] / processing_stats[\"finbert_calls\"]\n",
    "        \n",
    "        return {\n",
    "            'finbert_label': predicted_label,\n",
    "            'finbert_score': python_round(score, 4),\n",
    "            'finbert_confidence': python_round(confidence, 4),\n",
    "            'finbert_negative': python_round(float(predictions[0]), 4),\n",
    "            'finbert_neutral': python_round(float(predictions[1]), 4),\n",
    "            'finbert_positive': python_round(float(predictions[2]), 4),\n",
    "            'processing_method': 'finbert_production',\n",
    "            'processing_time_ms': python_round(processing_time * 1000, 2)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        processing_stats[\"finbert_calls\"] += 1\n",
    "        processing_stats[\"total_time\"] += processing_time\n",
    "        \n",
    "        return {\n",
    "            'finbert_label': 'neutral',\n",
    "            'finbert_score': 0.0,\n",
    "            'finbert_confidence': 0.5,\n",
    "            'finbert_negative': 0.33,\n",
    "            'finbert_neutral': 0.34,\n",
    "            'finbert_positive': 0.33,\n",
    "            'processing_method': 'error_fallback',\n",
    "            'processing_time_ms': processing_time * 1000\n",
    "        }\n",
    "\n",
    "print(f\"\uD83E\uDD16 FinBERT Status: {'Production Ready' if finbert_ready else 'Enhanced Fallback'}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 4. Create Enhanced Silver Layer Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_enhanced_silver_tables():\n",
    "    \"\"\"Create Silver layer tables with explicit schemas\"\"\"\n",
    "    \n",
    "    print(\"\uD83C\uDFD7️ Creating enhanced Silver layer tables...\")\n",
    "    \n",
    "    # Create silver schema\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.silver\")\n",
    "    print(f\"✅ Silver schema created in catalog: {current_catalog}\")\n",
    "    \n",
    "    # Enhanced Stock Data Table\n",
    "    stock_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_stock_table} (\n",
    "            symbol STRING,\n",
    "            date DATE,\n",
    "            timestamp TIMESTAMP,\n",
    "            open_price DOUBLE,\n",
    "            high_price DOUBLE,\n",
    "            low_price DOUBLE,\n",
    "            close_price DOUBLE,\n",
    "            adjusted_close DOUBLE,\n",
    "            volume BIGINT,\n",
    "            split_coefficient DOUBLE,\n",
    "            dividend_amount DOUBLE,\n",
    "            \n",
    "            price_change DOUBLE,\n",
    "            price_change_pct DOUBLE,\n",
    "            daily_range DOUBLE,\n",
    "            daily_range_pct DOUBLE,\n",
    "            gap_pct DOUBLE,\n",
    "            \n",
    "            volume_change BIGINT,\n",
    "            volume_change_pct DOUBLE,\n",
    "            volume_ma_5d DOUBLE,\n",
    "            volume_ma_20d DOUBLE,\n",
    "            volume_ratio DOUBLE,\n",
    "            \n",
    "            sma_5d DOUBLE,\n",
    "            sma_10d DOUBLE,\n",
    "            sma_20d DOUBLE,\n",
    "            sma_50d DOUBLE,\n",
    "            ema_12d DOUBLE,\n",
    "            ema_26d DOUBLE,\n",
    "            \n",
    "            rsi_14d DOUBLE,\n",
    "            macd DOUBLE,\n",
    "            macd_signal DOUBLE,\n",
    "            macd_histogram DOUBLE,\n",
    "            bollinger_upper DOUBLE,\n",
    "            bollinger_lower DOUBLE,\n",
    "            bollinger_position DOUBLE,\n",
    "            \n",
    "            volatility_5d DOUBLE,\n",
    "            volatility_20d DOUBLE,\n",
    "            atr_14d DOUBLE,\n",
    "            \n",
    "            trend_direction STRING,\n",
    "            trend_strength DOUBLE,\n",
    "            support_level DOUBLE,\n",
    "            resistance_level DOUBLE,\n",
    "            \n",
    "            technical_signal STRING,\n",
    "            market_sentiment STRING,\n",
    "            momentum_indicator STRING,\n",
    "            \n",
    "            data_quality_score DOUBLE,\n",
    "            completeness_score DOUBLE,\n",
    "            reliability_score DOUBLE,\n",
    "            source STRING,\n",
    "            processing_timestamp TIMESTAMP,\n",
    "            ingestion_batch STRING,\n",
    "            processed_date DATE\n",
    "        ) USING DELTA\n",
    "        PARTITIONED BY (processed_date, symbol)\n",
    "        TBLPROPERTIES (\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true'\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(stock_table_sql)\n",
    "    print(f\"✅ Created enhanced stock table: {silver_stock_table}\")\n",
    "    \n",
    "    # Enhanced News Data Table\n",
    "    news_table_sql = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {silver_news_table} (\n",
    "            article_id STRING,\n",
    "            title STRING,\n",
    "            description STRING,\n",
    "            content STRING,\n",
    "            url STRING,\n",
    "            source STRING,\n",
    "            author STRING,\n",
    "            published_at TIMESTAMP,\n",
    "            published_date DATE,\n",
    "            \n",
    "            content_length INT,\n",
    "            title_length INT,\n",
    "            readability_score DOUBLE,\n",
    "            financial_relevance_score DOUBLE,\n",
    "            \n",
    "            mentioned_symbols ARRAY<STRING>,\n",
    "            financial_entities ARRAY<STRING>,\n",
    "            key_financial_terms ARRAY<STRING>,\n",
    "            named_entities ARRAY<STRING>,\n",
    "            \n",
    "            original_sentiment_indicators MAP<STRING, DOUBLE>,\n",
    "            \n",
    "            finbert_label STRING,\n",
    "            finbert_score DOUBLE,\n",
    "            finbert_confidence DOUBLE,\n",
    "            finbert_negative DOUBLE,\n",
    "            finbert_neutral DOUBLE,\n",
    "            finbert_positive DOUBLE,\n",
    "            finbert_processing_method STRING,\n",
    "            finbert_processing_time_ms DOUBLE,\n",
    "            \n",
    "            sentiment_intensity STRING,\n",
    "            sentiment_subjectivity DOUBLE,\n",
    "            emotional_tone STRING,\n",
    "            urgency_score DOUBLE,\n",
    "            \n",
    "            news_category STRING,\n",
    "            market_impact_category STRING,\n",
    "            time_sensitivity STRING,\n",
    "            \n",
    "            content_quality_score DOUBLE,\n",
    "            sentiment_quality_score DOUBLE,\n",
    "            overall_reliability_score DOUBLE,\n",
    "            \n",
    "            market_hours_flag BOOLEAN,\n",
    "            weekday_flag BOOLEAN,\n",
    "            earnings_season_flag BOOLEAN,\n",
    "            \n",
    "            processing_timestamp TIMESTAMP,\n",
    "            data_source STRING,\n",
    "            ingestion_batch STRING,\n",
    "            processed_date DATE\n",
    "        ) USING DELTA\n",
    "        PARTITIONED BY (processed_date, finbert_label)\n",
    "        TBLPROPERTIES (\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true'\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    spark.sql(news_table_sql)\n",
    "    print(f\"✅ Created enhanced news table: {silver_news_table}\")\n",
    "    \n",
    "    print(\"✅ Enhanced Silver layer tables created successfully\")\n",
    "\n",
    "create_enhanced_silver_tables()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 5. Comprehensive Technical Indicators\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_comprehensive_technical_indicators(stock_df):\n",
    "    \"\"\"Calculate comprehensive technical indicators with enhanced stability\"\"\"\n",
    "    \n",
    "    print(\"\uD83D\uDCCA Calculating comprehensive technical indicators...\")\n",
    "    \n",
    "    # Define window specifications\n",
    "    windows = {\n",
    "        'w5': Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-4, 0),\n",
    "        'w10': Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-9, 0),\n",
    "        'w20': Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-19, 0),\n",
    "        'w50': Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-49, 0),\n",
    "        'w14': Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-13, 0),\n",
    "        'lag': Window.partitionBy(\"symbol\").orderBy(\"date\")\n",
    "    }\n",
    "    \n",
    "    # Ensure proper data types\n",
    "    stock_df = stock_df.withColumn(\"close_price\", col(\"close_price\").cast(\"double\")) \\\n",
    "                     .withColumn(\"open_price\", col(\"open_price\").cast(\"double\")) \\\n",
    "                     .withColumn(\"high_price\", col(\"high_price\").cast(\"double\")) \\\n",
    "                     .withColumn(\"low_price\", col(\"low_price\").cast(\"double\")) \\\n",
    "                     .withColumn(\"volume\", col(\"volume\").cast(\"bigint\")) \\\n",
    "                     .withColumn(\"date\", col(\"date\").cast(\"date\")) \\\n",
    "                     .withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "    \n",
    "    # Basic price calculations with null handling\n",
    "    enhanced_df = stock_df.withColumn(\n",
    "        \"price_change\",\n",
    "        coalesce(col(\"close_price\") - lag(\"close_price\").over(windows['lag']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"price_change_pct\",\n",
    "        when(coalesce(lag(\"close_price\").over(windows['lag']), lit(0)) != 0,\n",
    "             (col(\"close_price\") - lag(\"close_price\").over(windows['lag'])) / \n",
    "             lag(\"close_price\").over(windows['lag']) * 100\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"daily_range\",\n",
    "        coalesce(col(\"high_price\") - col(\"low_price\"), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"daily_range_pct\",\n",
    "        when(coalesce(col(\"close_price\"), lit(0)) != 0,\n",
    "             (col(\"high_price\") - col(\"low_price\")) / col(\"close_price\") * 100\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"gap_pct\",\n",
    "        when(coalesce(lag(\"close_price\").over(windows['lag']), lit(0)) != 0,\n",
    "             (col(\"open_price\") - lag(\"close_price\").over(windows['lag'])) / \n",
    "             lag(\"close_price\").over(windows['lag']) * 100\n",
    "        ).otherwise(0.0)\n",
    "    )\n",
    "    \n",
    "    # Volume calculations\n",
    "    volume_df = enhanced_df.withColumn(\n",
    "        \"volume_change\",\n",
    "        coalesce(col(\"volume\") - lag(\"volume\").over(windows['lag']), lit(0))\n",
    "    ).withColumn(\n",
    "        \"volume_change_pct\",\n",
    "        when(coalesce(lag(\"volume\").over(windows['lag']), lit(0)) != 0,\n",
    "             (col(\"volume\") - lag(\"volume\").over(windows['lag'])) / \n",
    "             lag(\"volume\").over(windows['lag']).cast(\"double\") * 100\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"volume_ma_5d\",\n",
    "        coalesce(avg(\"volume\").over(windows['w5']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"volume_ma_20d\",\n",
    "        coalesce(avg(\"volume\").over(windows['w20']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"volume_ratio\",\n",
    "        when(coalesce(avg(\"volume\").over(windows['w20']), lit(0)) != 0,\n",
    "             col(\"volume\").cast(\"double\") / avg(\"volume\").over(windows['w20'])\n",
    "        ).otherwise(1.0)\n",
    "    )\n",
    "    \n",
    "    # Moving averages with null handling\n",
    "    ma_df = volume_df.withColumn(\n",
    "        \"sma_5d\",\n",
    "        coalesce(avg(\"close_price\").over(windows['w5']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"sma_10d\",\n",
    "        coalesce(avg(\"close_price\").over(windows['w10']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"sma_20d\",\n",
    "        coalesce(avg(\"close_price\").over(windows['w20']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"sma_50d\",\n",
    "        coalesce(avg(\"close_price\").over(windows['w50']), lit(0.0))\n",
    "    )\n",
    "    \n",
    "    # RSI calculation with stability\n",
    "    rsi_df = ma_df.withColumn(\n",
    "        \"gain\",\n",
    "        when(coalesce(col(\"price_change\"), lit(0)) > 0, col(\"price_change\")).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"loss\",\n",
    "        when(coalesce(col(\"price_change\"), lit(0)) < 0, -col(\"price_change\")).otherwise(0)\n",
    "    ).withColumn(\n",
    "        \"avg_gain\",\n",
    "        coalesce(avg(\"gain\").over(windows['w14']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"avg_loss\",\n",
    "        coalesce(avg(\"loss\").over(windows['w14']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"rsi_14d\",\n",
    "        when(coalesce(col(\"avg_loss\"), lit(0)) > 0, \n",
    "             100 - (100 / (1 + col(\"avg_gain\") / col(\"avg_loss\"))))\n",
    "        .otherwise(50.0)\n",
    "    )\n",
    "    \n",
    "    # Enhanced MACD and Bollinger Bands\n",
    "    enhanced_indicators_df = rsi_df.withColumn(\n",
    "        \"ema_12d\",\n",
    "        coalesce(avg(\"close_price\").over(Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-11, 0)), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"ema_26d\",\n",
    "        coalesce(avg(\"close_price\").over(Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-25, 0)), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"macd\",\n",
    "        coalesce(col(\"ema_12d\") - col(\"ema_26d\"), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"macd_signal\",\n",
    "        coalesce(avg(\"macd\").over(Window.partitionBy(\"symbol\").orderBy(\"date\").rowsBetween(-8, 0)), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"macd_histogram\",\n",
    "        coalesce(col(\"macd\") - col(\"macd_signal\"), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"bollinger_upper\",\n",
    "        coalesce(col(\"sma_20d\") + (2 * coalesce(stddev(\"close_price\").over(windows['w20']), lit(0))), col(\"sma_20d\"))\n",
    "    ).withColumn(\n",
    "        \"bollinger_lower\",\n",
    "        coalesce(col(\"sma_20d\") - (2 * coalesce(stddev(\"close_price\").over(windows['w20']), lit(0))), col(\"sma_20d\"))\n",
    "    ).withColumn(\n",
    "        \"bollinger_position\",\n",
    "        when((col(\"bollinger_upper\") - col(\"bollinger_lower\")) != 0,\n",
    "             (col(\"close_price\") - col(\"bollinger_lower\")) / \n",
    "             (col(\"bollinger_upper\") - col(\"bollinger_lower\"))\n",
    "        ).otherwise(0.5)\n",
    "    )\n",
    "    \n",
    "    # Additional volatility and support/resistance\n",
    "    volatility_df = enhanced_indicators_df.withColumn(\n",
    "        \"volatility_5d\",\n",
    "        coalesce(stddev(\"price_change_pct\").over(windows['w5']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"volatility_20d\",\n",
    "        coalesce(stddev(\"price_change_pct\").over(windows['w20']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"atr_14d\",\n",
    "        coalesce(avg(greatest(\n",
    "            col(\"high_price\") - col(\"low_price\"),\n",
    "            abs(col(\"high_price\") - coalesce(lag(\"close_price\").over(windows['lag']), col(\"close_price\"))),\n",
    "            abs(col(\"low_price\") - coalesce(lag(\"close_price\").over(windows['lag']), col(\"close_price\")))\n",
    "        )).over(windows['w14']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"support_level\",\n",
    "        coalesce(min(\"low_price\").over(windows['w20']), lit(0.0))\n",
    "    ).withColumn(\n",
    "        \"resistance_level\",\n",
    "        coalesce(max(\"high_price\").over(windows['w20']), lit(0.0))\n",
    "    )\n",
    "    \n",
    "    # Technical signals and quality scoring\n",
    "    final_df = volatility_df.withColumn(\n",
    "        \"trend_direction\",\n",
    "        when(coalesce(col(\"sma_5d\"), lit(0)) > coalesce(col(\"sma_20d\"), lit(0)), \"uptrend\")\n",
    "        .when(coalesce(col(\"sma_5d\"), lit(0)) < coalesce(col(\"sma_20d\"), lit(0)), \"downtrend\")\n",
    "        .otherwise(\"sideways\")\n",
    "    ).withColumn(\n",
    "        \"trend_strength\",\n",
    "        when(coalesce(col(\"sma_20d\"), lit(0)) != 0,\n",
    "             abs(col(\"sma_5d\") - col(\"sma_20d\")) / col(\"sma_20d\")\n",
    "        ).otherwise(0.0)\n",
    "    ).withColumn(\n",
    "        \"technical_signal\",\n",
    "        when((coalesce(col(\"rsi_14d\"), lit(50)) < 30) & (col(\"close_price\") < col(\"bollinger_lower\")), \"strong_buy\")\n",
    "        .when((coalesce(col(\"rsi_14d\"), lit(50)) < 40) & (col(\"trend_direction\") == \"uptrend\"), \"buy\")\n",
    "        .when((coalesce(col(\"rsi_14d\"), lit(50)) > 70) & (col(\"close_price\") > col(\"bollinger_upper\")), \"strong_sell\")\n",
    "        .when((coalesce(col(\"rsi_14d\"), lit(50)) > 60) & (col(\"trend_direction\") == \"downtrend\"), \"sell\")\n",
    "        .otherwise(\"hold\")\n",
    "    ).withColumn(\n",
    "        \"market_sentiment\",\n",
    "        when((coalesce(col(\"volume_ratio\"), lit(1)) > 2) & (coalesce(col(\"price_change_pct\"), lit(0)) > 3), \"strong_bullish\")\n",
    "        .when((coalesce(col(\"volume_ratio\"), lit(1)) > 1.5) & (coalesce(col(\"price_change_pct\"), lit(0)) > 1), \"bullish\")\n",
    "        .when((coalesce(col(\"volume_ratio\"), lit(1)) > 2) & (coalesce(col(\"price_change_pct\"), lit(0)) < -3), \"strong_bearish\")\n",
    "        .when((coalesce(col(\"volume_ratio\"), lit(1)) > 1.5) & (coalesce(col(\"price_change_pct\"), lit(0)) < -1), \"bearish\")\n",
    "        .otherwise(\"neutral\")\n",
    "    ).withColumn(\n",
    "        \"momentum_indicator\",\n",
    "        when((coalesce(col(\"macd_histogram\"), lit(0)) > 0) & (coalesce(col(\"rsi_14d\"), lit(50)) > 50), \"positive\")\n",
    "        .when((coalesce(col(\"macd_histogram\"), lit(0)) < 0) & (coalesce(col(\"rsi_14d\"), lit(50)) < 50), \"negative\")\n",
    "        .otherwise(\"neutral\")\n",
    "    ).withColumn(\n",
    "        \"completeness_score\",\n",
    "        when(col(\"sma_20d\").isNotNull() & col(\"rsi_14d\").isNotNull() & col(\"volume_ma_20d\").isNotNull(), 1.0)\n",
    "        .when(col(\"sma_5d\").isNotNull() & col(\"volume\").isNotNull(), 0.7)\n",
    "        .otherwise(0.3)\n",
    "    ).withColumn(\n",
    "        \"reliability_score\",\n",
    "        when(coalesce(col(\"volume\"), lit(0)) > 100000, 1.0)\n",
    "        .when(coalesce(col(\"volume\"), lit(0)) > 10000, 0.8)\n",
    "        .otherwise(0.5)\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Comprehensive technical indicators calculated with all required fields\")\n",
    "    return final_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 6. Main Combined Processing Pipeline\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def cleanup_existing_silver_tables():\n",
    "    \"\"\"Clean up existing silver tables to avoid schema conflicts\"\"\"\n",
    "    try:\n",
    "        print(\"\uD83E\uDDF9 Cleaning up existing Silver tables for fresh schema...\")\n",
    "        \n",
    "        # Drop existing tables if they exist\n",
    "        tables_to_clean = [silver_stock_table, silver_news_table]\n",
    "        \n",
    "        for table_name in tables_to_clean:\n",
    "            try:\n",
    "                spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "                print(f\"✅ Dropped existing table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Note: Could not drop {table_name}: {e}\")\n",
    "        \n",
    "        print(\"✅ Silver tables cleanup completed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Cleanup warning (proceeding anyway): {e}\")\n",
    "\n",
    "def process_combined_bronze_to_silver():\n",
    "    \"\"\"Combined pipeline with explicit schemas and comprehensive processing\"\"\"\n",
    "    \n",
    "    batch_id = f\"silver_combined_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    print(f\"\\n\uD83E\uDD48 Starting Combined Silver Layer Processing\")\n",
    "    print(f\"\uD83D\uDCCB Batch ID: {batch_id}\")\n",
    "    print(f\"⏰ Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"\uD83C\uDFAF Features: Explicit schemas + FinBERT + Technical indicators\")\n",
    "    \n",
    "    processing_metrics = {\n",
    "        'batch_id': batch_id,\n",
    "        'processing_date': datetime.now().date(),\n",
    "        'bronze_stock_records': 0,\n",
    "        'bronze_news_records': 0,\n",
    "        'silver_stock_records': 0,\n",
    "        'silver_news_records': 0,\n",
    "        'processing_errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Enhanced Stock Processing\n",
    "        print(f\"\\n\uD83D\uDCCA Phase 1: Enhanced Stock Processing\")\n",
    "        \n",
    "        try:\n",
    "            bronze_stock_df = spark.table(bronze_stock_table)\n",
    "            stock_count = bronze_stock_df.count()\n",
    "            processing_metrics['bronze_stock_records'] = stock_count\n",
    "            print(f\"\uD83D\uDCC8 Processing {stock_count:,} stock records\")\n",
    "            \n",
    "            if stock_count > 0:\n",
    "                # Apply technical indicators\n",
    "                enhanced_stock_df = calculate_comprehensive_technical_indicators(bronze_stock_df)\n",
    "                \n",
    "                # Prepare with explicit schema compliance\n",
    "                final_stock_df = enhanced_stock_df.select(\n",
    "                    col(\"symbol\").cast(\"string\"),\n",
    "                    col(\"date\").cast(\"date\"),\n",
    "                    col(\"timestamp\").cast(\"timestamp\"),\n",
    "                    col(\"open_price\").cast(\"double\"),\n",
    "                    col(\"high_price\").cast(\"double\"), \n",
    "                    col(\"low_price\").cast(\"double\"),\n",
    "                    col(\"close_price\").cast(\"double\"),\n",
    "                    # Ensure adjusted_close is always double type\n",
    "                    coalesce(col(\"adjusted_close\"), col(\"close_price\")).cast(\"double\").alias(\"adjusted_close\"),\n",
    "                    col(\"volume\").cast(\"bigint\"),\n",
    "                    coalesce(col(\"split_coefficient\"), lit(1.0)).cast(\"double\").alias(\"split_coefficient\"),\n",
    "                    coalesce(col(\"dividend_amount\"), lit(0.0)).cast(\"double\").alias(\"dividend_amount\"),\n",
    "                    \n",
    "                    # Price indicators with explicit casting\n",
    "                    coalesce(col(\"price_change\"), lit(0.0)).cast(\"double\").alias(\"price_change\"),\n",
    "                    coalesce(col(\"price_change_pct\"), lit(0.0)).cast(\"double\").alias(\"price_change_pct\"),\n",
    "                    coalesce(col(\"daily_range\"), lit(0.0)).cast(\"double\").alias(\"daily_range\"),\n",
    "                    coalesce(col(\"daily_range_pct\"), lit(0.0)).cast(\"double\").alias(\"daily_range_pct\"),\n",
    "                    coalesce(col(\"gap_pct\"), lit(0.0)).cast(\"double\").alias(\"gap_pct\"),\n",
    "                    \n",
    "                    # Volume indicators with explicit casting\n",
    "                    coalesce(col(\"volume_change\"), lit(0)).cast(\"bigint\").alias(\"volume_change\"),\n",
    "                    coalesce(col(\"volume_change_pct\"), lit(0.0)).cast(\"double\").alias(\"volume_change_pct\"),\n",
    "                    coalesce(col(\"volume_ma_5d\"), lit(0.0)).cast(\"double\").alias(\"volume_ma_5d\"),\n",
    "                    coalesce(col(\"volume_ma_20d\"), lit(0.0)).cast(\"double\").alias(\"volume_ma_20d\"),\n",
    "                    coalesce(col(\"volume_ratio\"), lit(1.0)).cast(\"double\").alias(\"volume_ratio\"),\n",
    "                    \n",
    "                    # Moving averages with explicit casting\n",
    "                    coalesce(col(\"sma_5d\"), lit(0.0)).cast(\"double\").alias(\"sma_5d\"),\n",
    "                    coalesce(col(\"sma_10d\"), lit(0.0)).cast(\"double\").alias(\"sma_10d\"),\n",
    "                    coalesce(col(\"sma_20d\"), lit(0.0)).cast(\"double\").alias(\"sma_20d\"),\n",
    "                    coalesce(col(\"sma_50d\"), lit(0.0)).cast(\"double\").alias(\"sma_50d\"),\n",
    "                    coalesce(col(\"ema_12d\"), lit(0.0)).cast(\"double\").alias(\"ema_12d\"),\n",
    "                    coalesce(col(\"ema_26d\"), lit(0.0)).cast(\"double\").alias(\"ema_26d\"),\n",
    "                    \n",
    "                    # Technical indicators with explicit casting\n",
    "                    coalesce(col(\"rsi_14d\"), lit(50.0)).cast(\"double\").alias(\"rsi_14d\"),\n",
    "                    coalesce(col(\"macd\"), lit(0.0)).cast(\"double\").alias(\"macd\"),\n",
    "                    coalesce(col(\"macd_signal\"), lit(0.0)).cast(\"double\").alias(\"macd_signal\"),\n",
    "                    coalesce(col(\"macd_histogram\"), lit(0.0)).cast(\"double\").alias(\"macd_histogram\"),\n",
    "                    coalesce(col(\"bollinger_upper\"), lit(0.0)).cast(\"double\").alias(\"bollinger_upper\"),\n",
    "                    coalesce(col(\"bollinger_lower\"), lit(0.0)).cast(\"double\").alias(\"bollinger_lower\"),\n",
    "                    coalesce(col(\"bollinger_position\"), lit(0.5)).cast(\"double\").alias(\"bollinger_position\"),\n",
    "                    \n",
    "                    # Volatility measures with explicit casting\n",
    "                    coalesce(col(\"volatility_5d\"), lit(0.0)).cast(\"double\").alias(\"volatility_5d\"),\n",
    "                    coalesce(col(\"volatility_20d\"), lit(0.0)).cast(\"double\").alias(\"volatility_20d\"),\n",
    "                    coalesce(col(\"atr_14d\"), lit(0.0)).cast(\"double\").alias(\"atr_14d\"),\n",
    "                    \n",
    "                    # Trend analysis with explicit casting\n",
    "                    coalesce(col(\"trend_direction\"), lit(\"sideways\")).cast(\"string\").alias(\"trend_direction\"),\n",
    "                    coalesce(col(\"trend_strength\"), lit(0.0)).cast(\"double\").alias(\"trend_strength\"),\n",
    "                    coalesce(col(\"support_level\"), lit(0.0)).cast(\"double\").alias(\"support_level\"),\n",
    "                    coalesce(col(\"resistance_level\"), lit(0.0)).cast(\"double\").alias(\"resistance_level\"),\n",
    "                    \n",
    "                    # Signal generation with explicit casting\n",
    "                    coalesce(col(\"technical_signal\"), lit(\"hold\")).cast(\"string\").alias(\"technical_signal\"),\n",
    "                    coalesce(col(\"market_sentiment\"), lit(\"neutral\")).cast(\"string\").alias(\"market_sentiment\"),\n",
    "                    coalesce(col(\"momentum_indicator\"), lit(\"neutral\")).cast(\"string\").alias(\"momentum_indicator\"),\n",
    "                    \n",
    "                    # Quality metrics with explicit casting\n",
    "                    coalesce(col(\"data_quality_score\"), lit(1.0)).cast(\"double\").alias(\"data_quality_score\"),\n",
    "                    coalesce(col(\"completeness_score\"), lit(0.8)).cast(\"double\").alias(\"completeness_score\"),\n",
    "                    coalesce(col(\"reliability_score\"), lit(0.8)).cast(\"double\").alias(\"reliability_score\"),\n",
    "                    \n",
    "                    # Processing metadata with explicit casting\n",
    "                    lit(bronze_stock_table).cast(\"string\").alias(\"source\"),\n",
    "                    current_timestamp().alias(\"processing_timestamp\"),\n",
    "                    lit(batch_id).cast(\"string\").alias(\"ingestion_batch\"),\n",
    "                    current_date().alias(\"processed_date\")\n",
    "                )\n",
    "                \n",
    "                # Type-safe save with overwrite mode to avoid schema conflicts\n",
    "                final_stock_df.write \\\n",
    "                    .format(\"delta\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .option(\"mergeSchema\", \"false\") \\\n",
    "                    .option(\"overwriteSchema\", \"true\") \\\n",
    "                    .saveAsTable(silver_stock_table)\n",
    "                \n",
    "                processing_metrics['silver_stock_records'] = final_stock_df.count()\n",
    "                print(f\"✅ Created {processing_metrics['silver_stock_records']:,} enhanced stock records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Stock processing error: {str(e)}\"\n",
    "            processing_metrics['processing_errors'].append(error_msg)\n",
    "            print(f\"❌ {error_msg}\")\n",
    "        \n",
    "        # Phase 2: Enhanced News Processing with FinBERT\n",
    "        print(f\"\\n\uD83E\uDD16 Phase 2: Enhanced News Processing with FinBERT\")\n",
    "        \n",
    "        try:\n",
    "            bronze_news_df = spark.table(bronze_news_table)\n",
    "            news_count = bronze_news_df.count()\n",
    "            processing_metrics['bronze_news_records'] = news_count\n",
    "            print(f\"\uD83D\uDCF0 Found {news_count:,} news records\")\n",
    "            \n",
    "            if news_count > 0:\n",
    "                # Filter and limit for stable processing\n",
    "                filtered_df = bronze_news_df.filter(\n",
    "                    (col(\"title\").isNotNull()) & \n",
    "                    (length(col(\"title\")) > 5)\n",
    "                ).limit(SILVER_CONFIG['max_news_records_per_batch'])\n",
    "                \n",
    "                records = filtered_df.collect()\n",
    "                print(f\"\uD83D\uDD04 Processing {len(records)} high-quality news records with FinBERT...\")\n",
    "                \n",
    "                processed_records = []\n",
    "                \n",
    "                for i, record in enumerate(records):\n",
    "                    try:\n",
    "                        # Handle record attributes safely\n",
    "                        try:\n",
    "                            title = record['title'] if 'title' in record else record.title if hasattr(record, 'title') else \"\"\n",
    "                            description = record['description'] if 'description' in record else record.description if hasattr(record, 'description') else \"\"\n",
    "                            content = record['content'] if 'content' in record else record.content if hasattr(record, 'content') else \"\"\n",
    "                            \n",
    "                            # Safely access other fields\n",
    "                            article_id = record['article_id'] if 'article_id' in record else record.article_id if hasattr(record, 'article_id') else f\"combined_{i}\"\n",
    "                            url = record['url'] if 'url' in record else record.url if hasattr(record, 'url') else \"\"\n",
    "                            source = record['source'] if 'source' in record else record.source if hasattr(record, 'source') else \"unknown\"\n",
    "                            author = record['author'] if 'author' in record else record.author if hasattr(record, 'author') else \"unknown\"\n",
    "                            \n",
    "                            # Handle readability and financial relevance scores\n",
    "                            try:\n",
    "                                readability_score = float(record['readability_score']) if 'readability_score' in record and record['readability_score'] is not None else 0.5\n",
    "                            except:\n",
    "                                readability_score = 0.5\n",
    "                                \n",
    "                            try:\n",
    "                                financial_relevance_score = float(record['financial_relevance_score']) if 'financial_relevance_score' in record and record['financial_relevance_score'] is not None else 0.5\n",
    "                            except:\n",
    "                                financial_relevance_score = 0.5\n",
    "                                \n",
    "                        except Exception as attr_error:\n",
    "                            print(f\"⚠️ Record attribute access error for record {i}: {attr_error}\")\n",
    "                            continue\n",
    "                        full_text = f\"{title}. {description}. {content}\"\n",
    "                        \n",
    "                        # Apply FinBERT\n",
    "                        finbert_result = apply_production_finbert_sentiment(full_text)\n",
    "                        \n",
    "                        # Handle timestamp parsing safely\n",
    "                        try:\n",
    "                            published_at_raw = record['published_at'] if 'published_at' in record else record.published_at if hasattr(record, 'published_at') else None\n",
    "                            if published_at_raw:\n",
    "                                if isinstance(published_at_raw, str):\n",
    "                                    published_at = datetime.fromisoformat(published_at_raw.replace('Z', '+00:00'))\n",
    "                                else:\n",
    "                                    published_at = published_at_raw\n",
    "                                published_date = published_at.date() if hasattr(published_at, 'date') else datetime.now().date()\n",
    "                            else:\n",
    "                                published_at = datetime.now(timezone.utc)\n",
    "                                published_date = datetime.now().date()\n",
    "                        except Exception as time_error:\n",
    "                            published_at = datetime.now(timezone.utc)\n",
    "                            published_date = datetime.now().date()\n",
    "                        \n",
    "                        # Create record with explicit type matching\n",
    "                        processed_record = (\n",
    "                            article_id[:200],  # article_id - truncated\n",
    "                            title[:500],  # title\n",
    "                            description[:1000],  # description\n",
    "                            content[:2000],  # content\n",
    "                            url[:500],  # url\n",
    "                            source[:100],  # source\n",
    "                            author[:200],  # author\n",
    "                            published_at,  # published_at\n",
    "                            published_date,  # published_date\n",
    "                            \n",
    "                            len(content),  # content_length\n",
    "                            len(title),  # title_length\n",
    "                            readability_score,  # readability_score\n",
    "                            financial_relevance_score,  # financial_relevance_score\n",
    "                            \n",
    "                            [],  # mentioned_symbols\n",
    "                            [],  # financial_entities\n",
    "                            [],  # key_financial_terms\n",
    "                            [],  # named_entities\n",
    "                            \n",
    "                            {},  # original_sentiment_indicators\n",
    "                            \n",
    "                            finbert_result['finbert_label'],  # finbert_label\n",
    "                            float(finbert_result['finbert_score']),  # finbert_score\n",
    "                            float(finbert_result['finbert_confidence']),  # finbert_confidence\n",
    "                            float(finbert_result['finbert_negative']),  # finbert_negative\n",
    "                            float(finbert_result['finbert_neutral']),  # finbert_neutral\n",
    "                            float(finbert_result['finbert_positive']),  # finbert_positive\n",
    "                            finbert_result['processing_method'],  # finbert_processing_method\n",
    "                            float(finbert_result['processing_time_ms']),  # finbert_processing_time_ms\n",
    "                            \n",
    "                            'medium',  # sentiment_intensity\n",
    "                            float(python_abs(finbert_result['finbert_score'])),  # sentiment_subjectivity\n",
    "                            finbert_result['finbert_label'],  # emotional_tone\n",
    "                            0.5,  # urgency_score\n",
    "                            \n",
    "                            'general',  # news_category\n",
    "                            'medium',  # market_impact_category\n",
    "                            'normal',  # time_sensitivity\n",
    "                            \n",
    "                            0.8,  # content_quality_score\n",
    "                            float(finbert_result['finbert_confidence']),  # sentiment_quality_score\n",
    "                            0.8,  # overall_reliability_score\n",
    "                            \n",
    "                            True,  # market_hours_flag\n",
    "                            True,  # weekday_flag\n",
    "                            False,  # earnings_season_flag\n",
    "                            \n",
    "                            datetime.now(timezone.utc),  # processing_timestamp\n",
    "                            bronze_news_table,  # data_source\n",
    "                            batch_id,  # ingestion_batch\n",
    "                            datetime.now().date()  # processed_date\n",
    "                        )\n",
    "                        \n",
    "                        processed_records.append(processed_record)\n",
    "                        \n",
    "                        if (i + 1) % 10 == 0:\n",
    "                            print(f\"\uD83D\uDD04 Processed {i + 1}/{len(records)} ({((i + 1)/len(records)*100):.1f}%)\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️ Error processing record {i}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                # Create DataFrame with explicit schema\n",
    "                if processed_records:\n",
    "                    print(f\"\uD83D\uDCBE Creating DataFrame with explicit schema for {len(processed_records)} records...\")\n",
    "                    \n",
    "                    news_df = spark.createDataFrame(processed_records, schema=get_enhanced_news_schema())\n",
    "                    \n",
    "                    # Type-safe save with overwrite to avoid schema conflicts\n",
    "                    news_df.write \\\n",
    "                        .format(\"delta\") \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"mergeSchema\", \"false\") \\\n",
    "                        .option(\"overwriteSchema\", \"true\") \\\n",
    "                        .saveAsTable(silver_news_table)\n",
    "                    \n",
    "                    processing_metrics['silver_news_records'] = len(processed_records)\n",
    "                    print(f\"✅ Created {len(processed_records):,} enhanced news records\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"News processing error: {str(e)}\"\n",
    "            processing_metrics['processing_errors'].append(error_msg)\n",
    "            print(f\"❌ {error_msg}\")\n",
    "        \n",
    "        # Phase 3: Optimization\n",
    "        print(f\"\\n⚡ Phase 3: Table Optimization\")\n",
    "        \n",
    "        try:\n",
    "            for table_name in [silver_stock_table, silver_news_table]:\n",
    "                try:\n",
    "                    table_count = spark.table(table_name).count()\n",
    "                    if table_count > 0:\n",
    "                        spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "                        print(f\"✅ Optimized {table_name} ({table_count:,} records)\")\n",
    "                except Exception as opt_error:\n",
    "                    print(f\"⚠️ Optimization warning: {opt_error}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Optimization phase warning: {e}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline error: {str(e)}\"\n",
    "        processing_metrics['processing_errors'].append(error_msg)\n",
    "        print(f\"❌ {error_msg}\")\n",
    "    \n",
    "    finally:\n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds() / 60\n",
    "        \n",
    "        print_combined_summary(processing_metrics, start_time, end_time, duration)\n",
    "    \n",
    "    return processing_metrics\n",
    "\n",
    "def print_combined_summary(metrics, start_time, end_time, duration_minutes):\n",
    "    \"\"\"Print comprehensive processing summary\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\uD83E\uDD48 SILVER LAYER PROCESSING SUMMARY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\uD83D\uDCCB Processing Details:\")\n",
    "    print(f\"   Batch ID: {metrics['batch_id']}\")\n",
    "    print(f\"   Duration: {duration_minutes:.2f} minutes\")\n",
    "    print(f\"   Start: {start_time.strftime('%H:%M:%S')}\")\n",
    "    print(f\"   End: {end_time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    total_silver = metrics['silver_stock_records'] + metrics['silver_news_records']\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCCA Processing Results:\")\n",
    "    print(f\"   \uD83D\uDCC8 Stock Records: {metrics['bronze_stock_records']:,} → {metrics['silver_stock_records']:,}\")\n",
    "    print(f\"   \uD83D\uDCF0 News Records: {metrics['bronze_news_records']:,} → {metrics['silver_news_records']:,}\")\n",
    "    print(f\"   \uD83C\uDFAF Total Enhanced: {total_silver:,} records\")\n",
    "    \n",
    "    print(f\"\\n\uD83E\uDD16 FinBERT Processing:\")\n",
    "    print(f\"   Model Status: {'Production' if finbert_ready else 'Enhanced Fallback'}\")\n",
    "    print(f\"   Total Calls: {processing_stats['finbert_calls']:,}\")\n",
    "    print(f\"   Avg Time: {processing_stats.get('avg_time_per_call', 0)*1000:.2f}ms\")\n",
    "    \n",
    "    print(f\"\\n\uD83C\uDFAF Enhanced Features Created:\")\n",
    "    print(f\"   ✅ Technical indicators (RSI, MACD, Bollinger Bands)\")\n",
    "    print(f\"   ✅ FinBERT sentiment analysis with confidence\")\n",
    "    print(f\"   ✅ Type-safe DataFrame creation with explicit schemas\")\n",
    "    print(f\"   ✅ Quality scoring and reliability metrics\")\n",
    "    print(f\"   ✅ Optimized for correlation analysis\")\n",
    "    \n",
    "    if metrics['processing_errors']:\n",
    "        print(f\"\\n⚠️ Processing Issues ({len(metrics['processing_errors'])}):\")\n",
    "        for error in metrics['processing_errors'][:2]:\n",
    "            print(f\"   • {error}\")\n",
    "    \n",
    "    if total_silver > 0:\n",
    "        print(f\"\\n✅ STATUS: SUCCESS - Ready for Gold layer correlation analysis!\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ STATUS: Review Bronze layer data availability\")\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 7. Execute Combined Processing Pipeline\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Execute the combined processing\n",
    "try:\n",
    "    print(f\"\uD83D\uDE80 Launching Silver Layer Processing\")\n",
    "    print(f\"\uD83C\uDFAF Key Features:\")\n",
    "    print(f\"   • Explicit schemas for type safety\")\n",
    "    print(f\"   • Production FinBERT sentiment analysis\")\n",
    "    print(f\"   • Comprehensive technical indicators\")\n",
    "    print(f\"   • Enhanced error handling and stability\")\n",
    "    \n",
    "    # Clean up existing tables first\n",
    "    cleanup_existing_silver_tables()\n",
    "    \n",
    "    processing_results = process_combined_bronze_to_silver()\n",
    "    \n",
    "    print(f\"\\n\uD83C\uDF89 Combined Silver Layer Processing Complete!\")\n",
    "    \n",
    "    # Final verification\n",
    "    try:\n",
    "        stock_count = spark.table(silver_stock_table).count() if spark.catalog.tableExists(silver_stock_table) else 0\n",
    "        news_count = spark.table(silver_news_table).count() if spark.catalog.tableExists(silver_news_table) else 0\n",
    "        \n",
    "        print(f\"\\n\uD83D\uDCCA Final Silver Layer Status:\")\n",
    "        print(f\"   \uD83D\uDCC8 Enhanced Stock Records: {stock_count:,}\")\n",
    "        print(f\"   \uD83D\uDCF0 Enhanced News Records: {news_count:,}\")\n",
    "        \n",
    "        if stock_count > 0 and news_count > 0:\n",
    "            print(f\"\\n\uD83D\uDD17 Correlation Analysis Ready:\")\n",
    "            print(f\"   ✅ Stock technical indicators available\")\n",
    "            print(f\"   ✅ News sentiment analysis complete\")\n",
    "            print(f\"   ✅ Both datasets in Silver layer\")\n",
    "            print(f\"   \uD83D\uDE80 Ready for Gold layer processing!\")\n",
    "            \n",
    "            # Show preview of correlation readiness\n",
    "            print(f\"\\n\uD83D\uDCCA Stock Data Preview:\")\n",
    "            spark.table(silver_stock_table) \\\n",
    "                .select(\"symbol\", \"date\", \"close_price\", \"rsi_14d\", \"technical_signal\") \\\n",
    "                .orderBy(\"date\", \"symbol\") \\\n",
    "                .limit(3) \\\n",
    "                .show()\n",
    "            \n",
    "            print(f\"\\n\uD83D\uDCF0 News Data Preview:\")\n",
    "            spark.table(silver_news_table) \\\n",
    "                .select(\"title\", \"finbert_label\", \"finbert_score\", \"finbert_confidence\") \\\n",
    "                .orderBy(col(\"processing_timestamp\").desc()) \\\n",
    "                .limit(3) \\\n",
    "                .show(truncate=False)\n",
    "                \n",
    "    except Exception as verification_error:\n",
    "        print(f\"⚠️ Verification warning: {verification_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Combined processing execution failed: {e}\")\n",
    "    print(f\"\uD83D\uDCCB Full error traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## 8. Final Verification and Readiness Assessment\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def final_silver_verification():\n",
    "    \"\"\"Comprehensive verification of Silver layer readiness\"\"\"\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDD0D Final Silver Layer Verification\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    verification_results = {\n",
    "        'stock_ready': False,\n",
    "        'news_ready': False,\n",
    "        'correlation_ready': False,\n",
    "        'quality_score': 0.0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Stock data verification\n",
    "        print(f\"\\n\uD83D\uDCCA Stock Data Verification:\")\n",
    "        try:\n",
    "            stock_df = spark.table(silver_stock_table)\n",
    "            stock_count = stock_df.count()\n",
    "            \n",
    "            if stock_count > 0:\n",
    "                # Check technical indicators\n",
    "                tech_completeness = stock_df.agg(\n",
    "                    avg(when(col(\"rsi_14d\").isNotNull(), 1.0).otherwise(0.0)).alias(\"rsi_complete\"),\n",
    "                    avg(when(col(\"macd\").isNotNull(), 1.0).otherwise(0.0)).alias(\"macd_complete\"),\n",
    "                    avg(when(col(\"bollinger_upper\").isNotNull(), 1.0).otherwise(0.0)).alias(\"bb_complete\")\n",
    "                ).collect()[0]\n",
    "                \n",
    "                stock_quality = (tech_completeness['rsi_complete'] + \n",
    "                               tech_completeness['macd_complete'] + \n",
    "                               tech_completeness['bb_complete']) / 3\n",
    "                \n",
    "                print(f\"   \uD83D\uDCC8 Records: {stock_count:,}\")\n",
    "                print(f\"   \uD83D\uDCCA Technical Indicators Quality: {stock_quality:.2%}\")\n",
    "                \n",
    "                if stock_quality > 0.7:\n",
    "                    verification_results['stock_ready'] = True\n",
    "                    print(f\"   ✅ Stock data ready for correlation analysis\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ Stock data quality needs improvement\")\n",
    "            else:\n",
    "                print(f\"   ❌ No stock records found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Stock verification error: {e}\")\n",
    "        \n",
    "        # News data verification\n",
    "        print(f\"\\n\uD83D\uDCF0 News Data Verification:\")\n",
    "        try:\n",
    "            news_df = spark.table(silver_news_table)\n",
    "            news_count = news_df.count()\n",
    "            \n",
    "            if news_count > 0:\n",
    "                # Check FinBERT completeness\n",
    "                sentiment_stats = news_df.agg(\n",
    "                    avg(when(col(\"finbert_confidence\") >= 0.6, 1.0).otherwise(0.0)).alias(\"high_confidence\"),\n",
    "                    countDistinct(\"finbert_label\").alias(\"sentiment_variety\"),\n",
    "                    avg(\"finbert_confidence\").alias(\"avg_confidence\")\n",
    "                ).collect()[0]\n",
    "                \n",
    "                print(f\"   \uD83D\uDCF0 Records: {news_count:,}\")\n",
    "                print(f\"   \uD83E\uDD16 High Confidence Predictions: {sentiment_stats['high_confidence']:.2%}\")\n",
    "                print(f\"   \uD83D\uDCCA Average FinBERT Confidence: {sentiment_stats['avg_confidence']:.3f}\")\n",
    "                print(f\"   \uD83C\uDFAF Sentiment Variety: {sentiment_stats['sentiment_variety']} labels\")\n",
    "                \n",
    "                if sentiment_stats['high_confidence'] > 0.5 and sentiment_stats['sentiment_variety'] >= 2:\n",
    "                    verification_results['news_ready'] = True\n",
    "                    print(f\"   ✅ News data ready for correlation analysis\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ News data quality needs improvement\")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"   ❌ No news records found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ News verification error: {e}\")\n",
    "        \n",
    "        # Correlation readiness assessment\n",
    "        print(f\"\\n\uD83D\uDD17 Correlation Analysis Readiness:\")\n",
    "        \n",
    "        if verification_results['stock_ready'] and verification_results['news_ready']:\n",
    "            verification_results['correlation_ready'] = True\n",
    "            verification_results['quality_score'] = 0.9\n",
    "            \n",
    "            print(f\"   ✅ Both datasets are ready for correlation analysis\")\n",
    "            print(f\"   ✅ Technical indicators calculated and validated\")\n",
    "            print(f\"   ✅ FinBERT sentiment analysis completed\")\n",
    "            print(f\"   ✅ Data quality thresholds met\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ⚠️ Prerequisites not fully met for correlation analysis\")\n",
    "            if not verification_results['stock_ready']:\n",
    "                print(f\"   • Stock data quality needs improvement\")\n",
    "            if not verification_results['news_ready']:\n",
    "                print(f\"   • News sentiment analysis needs improvement\")\n",
    "        \n",
    "        print(f\"\\n\uD83D\uDCCA Overall Silver Layer Quality Score: {verification_results['quality_score']:.1%}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Verification failed: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return verification_results\n",
    "\n",
    "# Run final verification\n",
    "verification_results = final_silver_verification()\n",
    "\n",
    "print(f\"\\n\uD83C\uDF8A SILVER LAYER PROCESSOR COMPLETED\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if verification_results['correlation_ready']:\n",
    "    print(f\"\uD83C\uDFAF STATUS: READY FOR ADVANCED ANALYTICS\")\n",
    "else:\n",
    "    print(f\"⚠️ STATUS: REVIEW REQUIRED\")\n",
    "    print(f\"\uD83D\uDCA1 Check data quality and processing configuration\")\n",
    "\n",
    "print(f\"\\n⏰ Processing completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9334e57-05a1-4faa-a737-3f668c1e850f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7398733180968179>, line 11\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# If we reach here, notebook executed successfully\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m     success_result \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m      9\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSUCCESS\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     10\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNotebook execution completed successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m---> 11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_id,\n",
       "\u001B[1;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecution_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m: datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(),\n",
       "\u001B[1;32m     13\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords_processed\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlocals\u001B[39m()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_records_processed\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m),  \u001B[38;5;66;03m# Update based on your variables\u001B[39;00m\n",
       "\u001B[1;32m     14\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_quality_score\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlocals\u001B[39m()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_quality_score\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1.0\u001B[39m)     \u001B[38;5;66;03m# Update based on your variables\u001B[39;00m\n",
       "\u001B[1;32m     15\u001B[0m     }\n",
       "\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Notebook Success:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'batch_id' is not defined\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7398733180968179>, line 28\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m     dbutils\u001B[38;5;241m.\u001B[39mnotebook\u001B[38;5;241m.\u001B[39mexit(success_result)\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# If any error occurs, report failure\u001B[39;00m\n",
       "\u001B[1;32m     25\u001B[0m     failure_result \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m     26\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n",
       "\u001B[1;32m     27\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNotebook execution failed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_id,\n",
       "\u001B[1;32m     29\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecution_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m: datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(),\n",
       "\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(e)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n",
       "\u001B[1;32m     31\u001B[0m     }\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Notebook Failure:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(json\u001B[38;5;241m.\u001B[39mdumps(failure_result, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'batch_id' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'batch_id' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'batch_id' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-7398733180968179>, line 11\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# If we reach here, notebook executed successfully\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     success_result \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSUCCESS\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     10\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNotebook execution completed successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m---> 11\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_id,\n\u001B[1;32m     12\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecution_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m: datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(),\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecords_processed\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlocals\u001B[39m()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_records_processed\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m0\u001B[39m),  \u001B[38;5;66;03m# Update based on your variables\u001B[39;00m\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_quality_score\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mlocals\u001B[39m()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_quality_score\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m1.0\u001B[39m)     \u001B[38;5;66;03m# Update based on your variables\u001B[39;00m\n\u001B[1;32m     15\u001B[0m     }\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Notebook Success:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mNameError\u001B[0m: name 'batch_id' is not defined",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-7398733180968179>, line 28\u001B[0m\n\u001B[1;32m     21\u001B[0m     dbutils\u001B[38;5;241m.\u001B[39mnotebook\u001B[38;5;241m.\u001B[39mexit(success_result)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# If any error occurs, report failure\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     failure_result \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     26\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[1;32m     27\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNotebook execution failed: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m---> 28\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: batch_id,\n\u001B[1;32m     29\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecution_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m: datetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39misoformat(),\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(e)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[1;32m     31\u001B[0m     }\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m❌ Notebook Failure:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28mprint\u001B[39m(json\u001B[38;5;241m.\u001B[39mdumps(failure_result, indent\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n",
        "\u001B[0;31mNameError\u001B[0m: name 'batch_id' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Airflow Integration - Success/Failure Reporting\n",
    "\n",
    "try:\n",
    "    # If we reach here, notebook executed successfully\n",
    "    success_result = {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"message\": \"Notebook execution completed successfully\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"records_processed\": locals().get('total_records_processed', 0),  # Update based on your variables\n",
    "        \"data_quality_score\": locals().get('data_quality_score', 1.0)     # Update based on your variables\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Notebook Success:\")\n",
    "    print(json.dumps(success_result, indent=2))\n",
    "    \n",
    "    # Exit with success status for Airflow\n",
    "    dbutils.notebook.exit(success_result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # If any error occurs, report failure\n",
    "    failure_result = {\n",
    "        \"status\": \"FAILED\", \n",
    "        \"message\": f\"Notebook execution failed: {str(e)}\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"error_type\": type(e).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"❌ Notebook Failure:\")\n",
    "    print(json.dumps(failure_result, indent=2))\n",
    "    \n",
    "    # Exit with failure status for Airflow\n",
    "    dbutils.notebook.exit(failure_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_Layer_Processor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}