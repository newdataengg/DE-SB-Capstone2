{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a8b0ee1-0db4-4bbc-b634-65dfd92cfff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airflow Integration Parameters\n",
    "\n",
    "try:\n",
    "    # Create widgets for Airflow parameters\n",
    "    dbutils.widgets.text(\"batch_id\", \"manual_run\", \"Batch ID from Airflow\")\n",
    "    dbutils.widgets.text(\"execution_date\", \"\", \"Execution Date from Airflow\") \n",
    "    dbutils.widgets.text(\"force_refresh\", \"false\", \"Force data refresh\")\n",
    "    dbutils.widgets.text(\"quality_threshold\", \"0.8\", \"Data quality threshold\")\n",
    "    dbutils.widgets.text(\"dag_run_id\", \"\", \"DAG Run ID\")\n",
    "    \n",
    "    # Get parameter values\n",
    "    batch_id = dbutils.widgets.get(\"batch_id\")\n",
    "    execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "    force_refresh = dbutils.widgets.get(\"force_refresh\").lower() == \"true\"\n",
    "    quality_threshold = float(dbutils.widgets.get(\"quality_threshold\"))\n",
    "    dag_run_id = dbutils.widgets.get(\"dag_run_id\")\n",
    "    \n",
    "    print(f\"\uD83C\uDFAF Airflow Parameters:\")\n",
    "    print(f\"   Batch ID: {batch_id}\")\n",
    "    print(f\"   Execution Date: {execution_date}\")\n",
    "    print(f\"   Force Refresh: {force_refresh}\")\n",
    "    print(f\"   Quality Threshold: {quality_threshold}\")\n",
    "    print(f\"   DAG Run ID: {dag_run_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Widget creation failed (normal in some contexts): {e}\")\n",
    "    # Fallback values for manual runs\n",
    "    batch_id = \"manual_run\"\n",
    "    execution_date = \"\"\n",
    "    force_refresh = False\n",
    "    quality_threshold = 0.8\n",
    "    dag_run_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6e91fa-25ed-48be-bf16-a55e5ae9953d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCF0 Event Hub News Consumer with FinBERT\n============================================================\n\uD83C\uDFD7️ Setting up database schemas with ALL STRING types...\n\uD83D\uDD0D Checking existing schemas...\n\uD83D\uDCC1 Current catalog: databricks_stock_sentiment_canada\n\uD83D\uDCC1 Current database: default\n\uD83D\uDCCB Schema columns available: ['databaseName']\n\uD83D\uDCCB Existing schemas: ['bronze', 'default', 'information_schema', 'silver']\n✅ Silver schema already exists\n✅ Bronze schema already exists\n\uD83D\uDDD1️ Dropped any existing consumer table\n✅ Table databricks_stock_sentiment_canada.silver.news_data_consumer created with ALL STRING types\n\uD83D\uDCCB Available tables in silver schema: ['news_data_consumer', 'stock_data_consumer']\n\n============================================================\n\uD83D\uDCF0 Running News Consumer with FinBERT\n============================================================\n✅ Configuration loaded\n\uD83E\uDD16 Loading FinBERT model...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7adeda2bfe244f49a9d2506897a277d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21709da3d40643a8b7df0ac3b9bfcca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d127f8bfff4521a1eaabb94ef7275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d753803a95d84fb69a8a898cae660980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede3f67f7c4047af9fb1b1b752d71cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898707f395a4024ba588bf5d36bd3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FinBERT model ready\n\uD83D\uDE80 Starting News FinBERT Consumer Script\n⏰ 2025-07-21 22:13:46\n\n\uD83D\uDD04 Starting news data consumption and FinBERT processing...\n\uD83D\uDCF0 1: NEWS - Stock market today: Live updat...\n\uD83D\uDCF0 2: NEWS - Is It Time To Consider Buying ...\n\uD83D\uDCF0 3: NEWS - Your Photography Skills Are Al...\n\uD83D\uDCF0 4: NEWS - ASX Penny Stocks With Market C...\n\uD83D\uDCF0 5: NEWS - Is spinal cord stimulation saf...\n\uD83D\uDCF0 6: NEWS - ⚡ Nepal’s EV Revolution Surges...\n\uD83D\uDCF0 7: NEWS - High Growth Tech Stocks in Aus...\n\uD83D\uDCF0 8: NEWS - Stocks only go up…until they d...\n✅ Consumed 8 news messages\n\n\uD83D\uDD0D DEBUG - Message 1 structure:\n   Keys: ['title', 'content', 'source', 'published_at', 'url', 'sentiment_score', 'data_type', 'title_length', 'content_length', 'api_fetch_time', 'sentiment_category', 'event_timestamp', 'event_source', 'partition_key', 'processing_mode', 'consumer_timestamp', 'partition_id']\n   data_type: news_sentiment\n   title: Stock market today: Live updates...\n   title: Stock market today: Live updates... (type: str)\n   content: U.S. stock futures were little changed Sunday nigh... (type: str)\n   source: CNBC (type: str)\n   published_at: 2025-07-20T22:06:06Z (type: str)\n   url: https://www.cnbc.com/2025/07/20/stock-market-today-live-updates.html (type: str)\n   sentiment_score: 0.1 (type: str)\n   data_type: news_sentiment (type: str)\n   title_length: 32 (type: str)\n   content_length: 214 (type: str)\n   api_fetch_time: 2025-07-21T22:10:51.894156+00:00 (type: str)\n   sentiment_category: neutral (type: str)\n   event_timestamp: 2025-07-21T22:10:52.899417+00:00 (type: str)\n   event_source: api_producer (type: str)\n   partition_key: general (type: str)\n   processing_mode: streaming (type: str)\n   consumer_timestamp: 2025-07-21T22:13:52.309783+00:00 (type: str)\n   partition_id: 0 (type: str)\n\n\uD83E\uDD16 Processing 8 news events with FinBERT:\n\n\uD83D\uDD0D Processing news 1/8\n   Title: Stock market today: Live updates...\n✅ 1/8: Stock market today: Live updat... → positive (0.6853) [finbert]\n\n\uD83D\uDD0D Processing news 2/8\n   Title: Is It Time To Consider Buying Retail Food Group Li...\n✅ 2/8: Is It Time To Consider Buying ... → negative (-0.9418) [finbert]\n\n\uD83D\uDD0D Processing news 3/8\n   Title: Your Photography Skills Are Already Obsolete (You ...\n✅ 3/8: Your Photography Skills Are Al... → positive (0.5029) [finbert]\n\n\uD83D\uDD0D Processing news 4/8\n   Title: ASX Penny Stocks With Market Caps Over A$600M...\n✅ 4/8: ASX Penny Stocks With Market C... → negative (-0.9248) [finbert]\n\n\uD83D\uDD0D Processing news 5/8\n   Title: Is spinal cord stimulation safe? Does it work? Her...\n✅ 5/8: Is spinal cord stimulation saf... → positive (0.9438) [finbert]\n\n\uD83D\uDD0D Processing news 6/8\n   Title: ⚡ Nepal’s EV Revolution Surges Ahead as Economy Sh...\n✅ 6/8: ⚡ Nepal’s EV Revolution Surges... → negative (-0.8178) [finbert]\n\n\uD83D\uDD0D Processing news 7/8\n   Title: High Growth Tech Stocks in Australia Featuring Lif...\n✅ 7/8: High Growth Tech Stocks in Aus... → negative (-0.9396) [finbert]\n\n\uD83D\uDD0D Processing news 8/8\n   Title: Stocks only go up…until they don’t: A history less...\n✅ 8/8: Stocks only go up…until they d... → positive (0.7046) [finbert]\n\n\uD83D\uDCBE Saving 8 FinBERT processed news articles...\n\uD83D\uDCCB News DataFrame schema:\nroot\n |-- consumer_timestamp: string (nullable = true)\n |-- content: string (nullable = true)\n |-- finbert_confidence: string (nullable = true)\n |-- finbert_label: string (nullable = true)\n |-- finbert_negative: string (nullable = true)\n |-- finbert_neutral: string (nullable = true)\n |-- finbert_positive: string (nullable = true)\n |-- finbert_score: string (nullable = true)\n |-- ingestion_batch: string (nullable = true)\n |-- ingestion_source: string (nullable = true)\n |-- ingestion_time: string (nullable = true)\n |-- layer: string (nullable = true)\n |-- original_sentiment_category: string (nullable = true)\n |-- original_sentiment_score: string (nullable = true)\n |-- partition_id: string (nullable = true)\n |-- processed_date: string (nullable = true)\n |-- processing_method: string (nullable = true)\n |-- processing_mode: string (nullable = true)\n |-- processing_time_seconds: string (nullable = true)\n |-- published_at: string (nullable = true)\n |-- silver_processing_time: string (nullable = true)\n |-- source: string (nullable = true)\n |-- text_length: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n\n✅ Saved FinBERT news to Silver layer (ADLS)\n✅ Also saved news to Unity Catalog\n\n\uD83D\uDD0D Verification:\n✅ Silver layer news files: 2\n✅ Verified: 8 new FinBERT records (8 total)\n\n\uD83D\uDCCA Sample FinBERT Results:\n+--------------------------------------------------------------------------------------------------------------+-------------+-------------+------------------+-----------------+\n|title                                                                                                         |finbert_label|finbert_score|finbert_confidence|processing_method|\n+--------------------------------------------------------------------------------------------------------------+-------------+-------------+------------------+-----------------+\n|Stocks only go up…until they don’t: A history lesson and a forecast                                           |positive     |0.7046       |0.7046            |finbert          |\n|High Growth Tech Stocks in Australia Featuring Life360 and Two Others                                         |negative     |-0.9396      |0.9396            |finbert          |\n|⚡ Nepal’s EV Revolution Surges Ahead as Economy Shows Signs of Recovery – NRNA Elections Spark Diaspora Energy|negative     |-0.8178      |0.8178            |finbert          |\n|Is spinal cord stimulation safe? Does it work? Here’s what you need to know if you have back pain             |positive     |0.9438       |0.9438            |finbert          |\n|ASX Penny Stocks With Market Caps Over A$600M                                                                 |negative     |-0.9248      |0.9248            |finbert          |\n+--------------------------------------------------------------------------------------------------------------+-------------+-------------+------------------+-----------------+\n\n\n\uD83D\uDCC8 Sentiment Distribution:\n+-------------+-----+\n|finbert_label|count|\n+-------------+-----+\n|     negative|    4|\n|     positive|    4|\n+-------------+-----+\n\n\n\uD83D\uDCCB Final Summary:\n✅ Consumed: 8 news messages\n✅ Processed: 8 news articles with FinBERT\n✅ FinBERT model: Real model\n✅ ADLS save: True\n\uD83D\uDCC1 Batch ID: news_consumer_20250721_221415\n\uD83D\uDCCB New Table: databricks_stock_sentiment_canada.silver.news_data_consumer\n\n⏰ COMPLETED: 22:14:22\n"
     ]
    }
   ],
   "source": [
    "# Event Hub News Consumer with FinBERT Processing\n",
    "print(\"\uD83D\uDCF0 Event Hub News Consumer with FinBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store Python's built-in round before imports\n",
    "import builtins\n",
    "python_round = builtins.round\n",
    "\n",
    "# Import required modules\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timezone\n",
    "from azure.eventhub import EventHubConsumerClient, TransportType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import traceback\n",
    "\n",
    "# Enhanced schema setup with ALL STRING types to prevent ANY conflicts\n",
    "print(\"\uD83C\uDFD7️ Setting up database schemas with ALL STRING types...\")\n",
    "\n",
    "def create_schemas_and_tables():\n",
    "    \"\"\"Create catalog, schema, and tables for news data - ALL STRING TYPES\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\uD83D\uDD0D Checking existing schemas...\")\n",
    "        \n",
    "        # Get current catalog name\n",
    "        current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "        current_database = spark.sql(\"SELECT current_database()\").collect()[0][0]\n",
    "        \n",
    "        print(f\"\uD83D\uDCC1 Current catalog: {current_catalog}\")\n",
    "        print(f\"\uD83D\uDCC1 Current database: {current_database}\")\n",
    "        \n",
    "        # Show existing schemas with improved column access\n",
    "        try:\n",
    "            schemas_df = spark.sql(\"SHOW SCHEMAS\")\n",
    "            schema_columns = schemas_df.columns\n",
    "            print(f\"\uD83D\uDCCB Schema columns available: {schema_columns}\")\n",
    "            \n",
    "            if 'namespace' in schema_columns:\n",
    "                existing_schemas = [row['namespace'] for row in schemas_df.collect()]\n",
    "            elif 'databaseName' in schema_columns:\n",
    "                existing_schemas = [row['databaseName'] for row in schemas_df.collect()]\n",
    "            elif 'schemaName' in schema_columns:\n",
    "                existing_schemas = [row['schemaName'] for row in schemas_df.collect()]\n",
    "            else:\n",
    "                existing_schemas = [row[0] for row in schemas_df.collect()]\n",
    "            \n",
    "            print(f\"\uD83D\uDCCB Existing schemas: {existing_schemas}\")\n",
    "            \n",
    "        except Exception as schema_list_error:\n",
    "            print(f\"⚠️ Could not list schemas: {schema_list_error}\")\n",
    "            existing_schemas = []\n",
    "        \n",
    "        # Create silver schema if it doesn't exist\n",
    "        if 'silver' not in existing_schemas:\n",
    "            print(\"\uD83D\uDD27 Creating silver schema...\")\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.silver\")\n",
    "            print(\"✅ Silver schema created\")\n",
    "        else:\n",
    "            print(\"✅ Silver schema already exists\")\n",
    "            \n",
    "        # Create bronze schema if it doesn't exist (for backup verification)\n",
    "        if 'bronze' not in existing_schemas:\n",
    "            print(\"\uD83D\uDD27 Creating bronze schema...\")\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.bronze\")\n",
    "            print(\"✅ Bronze schema created\")\n",
    "        else:\n",
    "            print(\"✅ Bronze schema already exists\")\n",
    "        \n",
    "        # Create COMPLETELY NEW TABLE with ALL STRING types to avoid ALL conflicts\n",
    "        new_news_table = f\"{current_catalog}.silver.news_data_consumer\"\n",
    "        \n",
    "        # Drop existing table if it exists\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {new_news_table}\")\n",
    "            print(\"\uD83D\uDDD1️ Dropped any existing consumer table\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create silver news_data table with ALL STRING types\n",
    "        news_table_ddl = f\"\"\"\n",
    "        CREATE TABLE {new_news_table} (\n",
    "            title STRING,\n",
    "            content STRING,\n",
    "            source STRING,\n",
    "            published_at STRING,\n",
    "            url STRING,\n",
    "            original_sentiment_score STRING,     -- STRING to avoid precision conflicts\n",
    "            original_sentiment_category STRING,\n",
    "            finbert_label STRING,\n",
    "            finbert_score STRING,               -- STRING to avoid precision conflicts\n",
    "            finbert_confidence STRING,          -- STRING to avoid precision conflicts\n",
    "            finbert_negative STRING,            -- STRING to avoid precision conflicts\n",
    "            finbert_neutral STRING,             -- STRING to avoid precision conflicts\n",
    "            finbert_positive STRING,            -- STRING to avoid precision conflicts\n",
    "            processing_method STRING,\n",
    "            processing_time_seconds STRING,     -- STRING to avoid precision conflicts\n",
    "            text_length STRING,                 -- STRING to avoid INT/LONG conflicts\n",
    "            consumer_timestamp STRING,\n",
    "            partition_id STRING,\n",
    "            processing_mode STRING,\n",
    "            silver_processing_time STRING,\n",
    "            ingestion_time STRING,\n",
    "            processed_date STRING,\n",
    "            ingestion_batch STRING,\n",
    "            layer STRING,\n",
    "            ingestion_source STRING\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(news_table_ddl)\n",
    "        print(f\"✅ Table {new_news_table} created with ALL STRING types\")\n",
    "        \n",
    "        # Verify tables exist\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {current_catalog}.silver\").collect()\n",
    "        table_names = [row.tableName for row in tables]\n",
    "        print(f\"\uD83D\uDCCB Available tables in silver schema: {table_names}\")\n",
    "        \n",
    "        return current_catalog, new_news_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating schemas: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Create schemas and get catalog name\n",
    "current_catalog, news_table_name = create_schemas_and_tables()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\uD83D\uDCF0 Running News Consumer with FinBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "try:\n",
    "    eh_connection_string = dbutils.secrets.get(scope=\"stock-project\", key=\"event-hub-connection-string\")\n",
    "    if \"EntityPath=\" not in eh_connection_string:\n",
    "        eh_connection_string = f\"{eh_connection_string};EntityPath=stock-data-hub\"\n",
    "\n",
    "    storage_account_key = dbutils.secrets.get(scope=\"stock-project\", key=\"storage-account-key\")\n",
    "    storage_account_name = \"dlsstocksentiment2025\"\n",
    "    container_name = \"data\"\n",
    "\n",
    "    spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key)\n",
    "\n",
    "    adls_base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "    silver_news_path = f\"{adls_base_path}/silver/news_data_consumer\"\n",
    "\n",
    "    print(\"✅ Configuration loaded\")\n",
    "    \n",
    "except Exception as config_error:\n",
    "    print(f\"❌ Configuration error: {config_error}\")\n",
    "    raise\n",
    "\n",
    "# Load FinBERT with improved error handling\n",
    "finbert_ready = False\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\uD83E\uDD16 Loading FinBERT model...\")\n",
    "    device = torch.device('cpu')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "    model.eval()\n",
    "    finbert_ready = True\n",
    "    print(\"✅ FinBERT model ready\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ FinBERT failed, using fallback: {e}\")\n",
    "    finbert_ready = False\n",
    "\n",
    "def apply_finbert_sentiment(text):\n",
    "    \"\"\"Apply FinBERT sentiment analysis with robust fallback - RETURN ALL STRINGS\"\"\"\n",
    "    if not finbert_ready:\n",
    "        # Enhanced fallback sentiment analysis\n",
    "        text_lower = text.lower()\n",
    "        positive_words = ['gain', 'rise', 'profit', 'bullish', 'up', 'growth', 'strong', 'beat', 'outperform', 'surge']\n",
    "        negative_words = ['loss', 'fall', 'bearish', 'down', 'decline', 'weak', 'miss', 'underperform', 'drop', 'crash']\n",
    "        \n",
    "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        if positive_count > negative_count:\n",
    "            return {\n",
    "                'finbert_label': 'positive', \n",
    "                'finbert_score': '0.7', \n",
    "                'finbert_confidence': '0.8', \n",
    "                'finbert_negative': '0.1',\n",
    "                'finbert_neutral': '0.2',\n",
    "                'finbert_positive': '0.7',\n",
    "                'method': 'fallback'\n",
    "            }\n",
    "        elif negative_count > positive_count:\n",
    "            return {\n",
    "                'finbert_label': 'negative', \n",
    "                'finbert_score': '-0.7', \n",
    "                'finbert_confidence': '0.8',\n",
    "                'finbert_negative': '0.7',\n",
    "                'finbert_neutral': '0.2',\n",
    "                'finbert_positive': '0.1',\n",
    "                'method': 'fallback'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'finbert_label': 'neutral', \n",
    "                'finbert_score': '0.0', \n",
    "                'finbert_confidence': '0.7',\n",
    "                'finbert_negative': '0.33',\n",
    "                'finbert_neutral': '0.34',\n",
    "                'finbert_positive': '0.33',\n",
    "                'method': 'fallback'\n",
    "            }\n",
    "    \n",
    "    try:\n",
    "        # Limit text length and clean it\n",
    "        clean_text = text.strip()[:512]\n",
    "        if not clean_text:\n",
    "            clean_text = \"No content\"\n",
    "            \n",
    "        inputs = tokenizer(clean_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        predictions = predictions.numpy()[0]\n",
    "        labels = ['negative', 'neutral', 'positive']\n",
    "        \n",
    "        max_idx = np.argmax(predictions)\n",
    "        predicted_label = labels[max_idx]\n",
    "        confidence = float(predictions[max_idx])\n",
    "        \n",
    "        # Calculate score: positive for positive sentiment, negative for negative\n",
    "        if predicted_label == 'positive':\n",
    "            score = confidence\n",
    "        elif predicted_label == 'negative':\n",
    "            score = -confidence\n",
    "        else:\n",
    "            score = 0.0\n",
    "        \n",
    "        # RETURN ALL VALUES AS STRINGS\n",
    "        return {\n",
    "            'finbert_label': str(predicted_label),\n",
    "            'finbert_score': str(python_round(score, 4)),\n",
    "            'finbert_confidence': str(python_round(confidence, 4)),\n",
    "            'finbert_negative': str(python_round(float(predictions[0]), 4)),\n",
    "            'finbert_neutral': str(python_round(float(predictions[1]), 4)),\n",
    "            'finbert_positive': str(python_round(float(predictions[2]), 4)),\n",
    "            'method': 'finbert'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ FinBERT processing error: {e}\")\n",
    "        return {\n",
    "            'finbert_label': 'neutral', \n",
    "            'finbert_score': '0.0', \n",
    "            'finbert_confidence': '0.5',\n",
    "            'finbert_negative': '0.33',\n",
    "            'finbert_neutral': '0.34',\n",
    "            'finbert_positive': '0.33',\n",
    "            'method': 'error'\n",
    "        }\n",
    "\n",
    "def safe_save_to_catalog(df, table_name, mode=\"append\"):\n",
    "    \"\"\"Safely save to Unity Catalog with fallback\"\"\"\n",
    "    try:\n",
    "        (df.write\n",
    "         .format(\"delta\")\n",
    "         .mode(mode)\n",
    "         .option(\"mergeSchema\", \"false\")  # No schema merging to avoid conflicts\n",
    "         .saveAsTable(table_name))\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Unity Catalog save failed for {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def safe_parse_json(message_body):\n",
    "    \"\"\"Safely parse JSON with better error handling\"\"\"\n",
    "    try:\n",
    "        # Handle empty or whitespace-only messages\n",
    "        if not message_body or not message_body.strip():\n",
    "            print(f\"⚠️ Empty message body\")\n",
    "            return None\n",
    "            \n",
    "        # Handle non-JSON messages (like \"Test connection message\")\n",
    "        message_body = message_body.strip()\n",
    "        if not message_body.startswith('{') and not message_body.startswith('['):\n",
    "            print(f\"⚠️ Non-JSON message: {message_body[:50]}...\")\n",
    "            return None\n",
    "            \n",
    "        # Try to parse JSON\n",
    "        data = json.loads(message_body)\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as je:\n",
    "        print(f\"⚠️ JSON decode error: {je} - Raw message: {message_body[:100]}...\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Message parsing error: {e}\")\n",
    "        return None\n",
    "\n",
    "def debug_print_message_structure(event, index):\n",
    "    \"\"\"Debug function to print message structure\"\"\"\n",
    "    try:\n",
    "        print(f\"\\n\uD83D\uDD0D DEBUG - Message {index} structure:\")\n",
    "        print(f\"   Keys: {list(event.keys())}\")\n",
    "        print(f\"   data_type: {event.get('data_type', 'MISSING')}\")\n",
    "        print(f\"   title: {event.get('title', 'MISSING')[:50]}...\")\n",
    "        \n",
    "        # Print all key-value pairs for first few messages\n",
    "        if index <= 2:\n",
    "            for key, value in event.items():\n",
    "                if key in ['title', 'content']:\n",
    "                    print(f\"   {key}: {str(value)[:50]}... (type: {type(value).__name__})\")\n",
    "                else:\n",
    "                    print(f\"   {key}: {value} (type: {type(value).__name__})\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Debug print error: {e}\")\n",
    "\n",
    "def news_finbert_consume_and_process():\n",
    "    \"\"\"News consumer with FinBERT processing - ALL STRING TYPES\"\"\"\n",
    "    \n",
    "    print(\"\\n\uD83D\uDD04 Starting news data consumption and FinBERT processing...\")\n",
    "    \n",
    "    # Step 1: Consume messages with timeout and validation\n",
    "    messages = []\n",
    "    stop_flag = threading.Event()\n",
    "    \n",
    "    def consumer_thread():\n",
    "        try:\n",
    "            client = EventHubConsumerClient.from_connection_string(\n",
    "                eh_connection_string,\n",
    "                consumer_group=\"$Default\",\n",
    "                transport_type=TransportType.AmqpOverWebsocket\n",
    "            )\n",
    "            \n",
    "            def message_handler(partition_context, event):\n",
    "                if stop_flag.is_set() or len(messages) >= 12:\n",
    "                    return\n",
    "                \n",
    "                if event and hasattr(event, 'body_as_str'):\n",
    "                    try:\n",
    "                        body = event.body_as_str(encoding='UTF-8')\n",
    "                        \n",
    "                        # Use safe JSON parsing\n",
    "                        data = safe_parse_json(body)\n",
    "                        if data is None:\n",
    "                            return  # Skip invalid messages\n",
    "                        \n",
    "                        # Filter for news data only\n",
    "                        if data.get('data_type') == 'news_sentiment':\n",
    "                            data['consumer_timestamp'] = datetime.now(timezone.utc).isoformat()\n",
    "                            data['partition_id'] = partition_context.partition_id\n",
    "                            messages.append(data)\n",
    "                            title = data.get('title', 'No title')[:30]\n",
    "                            print(f\"\uD83D\uDCF0 {len(messages)}: NEWS - {title}...\")\n",
    "                            \n",
    "                            if len(messages) >= 12:\n",
    "                                stop_flag.set()\n",
    "                        else:\n",
    "                            # Skip non-news messages silently\n",
    "                            pass\n",
    "                            \n",
    "                    except Exception as me:\n",
    "                        print(f\"⚠️ Message processing error: {me}\")\n",
    "            \n",
    "            with client:\n",
    "                client.receive(\n",
    "                    on_event=message_handler,\n",
    "                    starting_position=\"-1\",\n",
    "                    max_wait_time=12\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Consumer error: {e}\")\n",
    "            stop_flag.set()\n",
    "    \n",
    "    # Start consumer thread\n",
    "    thread = threading.Thread(target=consumer_thread, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    # Wait with extended timeout\n",
    "    start_time = time.time()\n",
    "    timeout_seconds = 25\n",
    "    while thread.is_alive() and time.time() - start_time < timeout_seconds and len(messages) < 12:\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    stop_flag.set()\n",
    "    thread.join(timeout=3)\n",
    "    \n",
    "    print(f\"✅ Consumed {len(messages)} news messages\")\n",
    "    \n",
    "    if not messages:\n",
    "        print(\"⚠️ No news messages consumed - Event Hub might be empty or only contains stock data\")\n",
    "        return\n",
    "    \n",
    "    # Debug: Print first message structure\n",
    "    if messages:\n",
    "        debug_print_message_structure(messages[0], 1)\n",
    "    \n",
    "    # Step 2: Process news data with FinBERT - ALL STRING TYPES\n",
    "    news_events = [m for m in messages if m.get('data_type') == 'news_sentiment']\n",
    "    \n",
    "    print(f\"\\n\uD83E\uDD16 Processing {len(news_events)} news events with FinBERT:\")\n",
    "    \n",
    "    # Process news with enhanced FinBERT analysis - ALL STRING OUTPUT\n",
    "    processed_news = []\n",
    "    for i, event in enumerate(news_events):\n",
    "        try:\n",
    "            print(f\"\\n\uD83D\uDD0D Processing news {i+1}/{len(news_events)}\")\n",
    "            \n",
    "            title = str(event.get('title', '') or '')\n",
    "            content = str(event.get('content', '') or '')\n",
    "            \n",
    "            # Enhanced text preparation\n",
    "            if not title and not content:\n",
    "                title = f\"Article {i+1}\"\n",
    "                content = \"No content available\"\n",
    "            \n",
    "            full_text = f\"{title}. {content}\".strip()[:1000]  # Limit length\n",
    "            print(f\"   Title: {title[:50]}...\")\n",
    "            \n",
    "            # Apply FinBERT with timing\n",
    "            start_time = time.time()\n",
    "            result = apply_finbert_sentiment(full_text)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # ALL VALUES AS STRINGS to eliminate any type conflicts\n",
    "            processed_news.append({\n",
    "                'title': str(title[:500]) if title else \"\",\n",
    "                'content': str(content[:2000]) if content else \"\",\n",
    "                'source': str(event.get('source', 'unknown')),\n",
    "                'published_at': str(event.get('published_at', datetime.now(timezone.utc).isoformat())),\n",
    "                'url': str(event.get('url', '')[:500]),\n",
    "                'original_sentiment_score': str(event.get('sentiment_score', 0.0)),  # STRING\n",
    "                'original_sentiment_category': str(event.get('sentiment_category', 'neutral')),\n",
    "                'finbert_label': str(result['finbert_label']),\n",
    "                'finbert_score': str(result['finbert_score']),                      # STRING\n",
    "                'finbert_confidence': str(result['finbert_confidence']),            # STRING\n",
    "                'finbert_negative': str(result.get('finbert_negative', '0.33')),    # STRING\n",
    "                'finbert_neutral': str(result.get('finbert_neutral', '0.34')),      # STRING\n",
    "                'finbert_positive': str(result.get('finbert_positive', '0.33')),    # STRING\n",
    "                'processing_method': str(result['method']),\n",
    "                'processing_time_seconds': str(python_round(processing_time, 4)),   # STRING\n",
    "                'text_length': str(len(full_text)),                                 # STRING\n",
    "                'consumer_timestamp': str(event.get('consumer_timestamp', '')),\n",
    "                'partition_id': str(event.get('partition_id', '')),\n",
    "                'processing_mode': str('streaming_news_finbert'),\n",
    "                'silver_processing_time': str(datetime.now(timezone.utc).isoformat())\n",
    "            })\n",
    "            \n",
    "            print(f\"✅ {i+1}/{len(news_events)}: {title[:30]}... → {result['finbert_label']} ({result['finbert_score']}) [{result['method']}]\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ News processing error {i+1}: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Step 3: Save news data with ALL STRING metadata\n",
    "    batch_id = f\"news_consumer_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    save_success = False\n",
    "    \n",
    "    if processed_news:\n",
    "        try:\n",
    "            print(f\"\\n\uD83D\uDCBE Saving {len(processed_news)} FinBERT processed news articles...\")\n",
    "            \n",
    "            # Add metadata as strings\n",
    "            current_time_str = datetime.now(timezone.utc).isoformat()\n",
    "            current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            for record in processed_news:\n",
    "                record[\"ingestion_time\"] = str(current_time_str)\n",
    "                record[\"processed_date\"] = str(current_date_str)\n",
    "                record[\"ingestion_batch\"] = str(batch_id)\n",
    "                record[\"layer\"] = str(\"silver\")\n",
    "                record[\"ingestion_source\"] = str(\"news_consumer\")\n",
    "            \n",
    "            news_df = spark.createDataFrame(processed_news)\n",
    "            \n",
    "            # Debug: Show schema before saving\n",
    "            print(\"\uD83D\uDCCB News DataFrame schema:\")\n",
    "            news_df.printSchema()\n",
    "            \n",
    "            # Save to ADLS (primary) - NO SCHEMA MERGING\n",
    "            (news_df.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"append\")\n",
    "             .option(\"mergeSchema\", \"false\")\n",
    "             .save(silver_news_path))\n",
    "            \n",
    "            print(\"✅ Saved FinBERT news to Silver layer (ADLS)\")\n",
    "            save_success = True\n",
    "            \n",
    "            # Try Unity Catalog (secondary)\n",
    "            if current_catalog and news_table_name and safe_save_to_catalog(news_df, news_table_name):\n",
    "                print(\"✅ Also saved news to Unity Catalog\")\n",
    "            else:\n",
    "                print(\"⚠️ Unity Catalog news save skipped\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ News save error: {e}\")\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Step 4: Verification and reporting\n",
    "    print(f\"\\n\uD83D\uDD0D Verification:\")\n",
    "    try:\n",
    "        if save_success:\n",
    "            try:\n",
    "                # Verify ADLS storage\n",
    "                news_files = dbutils.fs.ls(silver_news_path)\n",
    "                print(f\"✅ Silver layer news files: {len(news_files)}\")\n",
    "                \n",
    "                # Verify data integrity\n",
    "                news_verify = spark.read.format(\"delta\").load(silver_news_path)\n",
    "                total_news = news_verify.count()\n",
    "                recent_news = news_verify.filter(col(\"ingestion_source\") == \"news_consumer\").count()\n",
    "                print(f\"✅ Verified: {recent_news} new FinBERT records ({total_news} total)\")\n",
    "                \n",
    "                # Show sample of what was saved\n",
    "                print(f\"\\n\uD83D\uDCCA Sample FinBERT Results:\")\n",
    "                (news_verify\n",
    "                 .filter(col(\"ingestion_source\") == \"news_consumer\")\n",
    "                 .select(\"title\", \"finbert_label\", \"finbert_score\", \"finbert_confidence\", \"processing_method\")\n",
    "                 .orderBy(col(\"silver_processing_time\").desc())\n",
    "                 .limit(5)\n",
    "                 .show(truncate=False))\n",
    "                \n",
    "                # Show sentiment distribution\n",
    "                print(f\"\\n\uD83D\uDCC8 Sentiment Distribution:\")\n",
    "                (news_verify\n",
    "                 .filter(col(\"ingestion_source\") == \"news_consumer\")\n",
    "                 .groupBy(\"finbert_label\")\n",
    "                 .agg(count(\"*\").alias(\"count\"))\n",
    "                 .orderBy(\"finbert_label\")\n",
    "                 .show())\n",
    "                \n",
    "            except Exception as ve:\n",
    "                print(f\"⚠️ News verification error: {ve}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Verification error: {e}\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n\uD83D\uDCCB Final Summary:\")\n",
    "    print(f\"✅ Consumed: {len(messages)} news messages\")\n",
    "    print(f\"✅ Processed: {len(processed_news)} news articles with FinBERT\")\n",
    "    print(f\"✅ FinBERT model: {'Real model' if finbert_ready else 'Enhanced fallback'}\")\n",
    "    print(f\"✅ ADLS save: {save_success}\")\n",
    "    print(f\"\uD83D\uDCC1 Batch ID: {batch_id}\")\n",
    "    print(f\"\uD83D\uDCCB New Table: {news_table_name}\")\n",
    "\n",
    "# Execute consumer with top-level error handling\n",
    "try:\n",
    "    print(\"\uD83D\uDE80 Starting News FinBERT Consumer Script\")\n",
    "    print(f\"⏰ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    news_finbert_consume_and_process()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ News FinBERT consumer failed: {e}\")\n",
    "    print(\"\uD83D\uDCCB Full error traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n⏰ COMPLETED: {datetime.now().strftime('%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de17643-09be-4542-b22c-92b577ee1f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Airflow Integration - Success/Failure Reporting\n",
    "\n",
    "try:\n",
    "    # If we reach here, notebook executed successfully\n",
    "    success_result = {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"message\": \"Notebook execution completed successfully\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"records_processed\": locals().get('total_records_processed', 0),  # Update based on your variables\n",
    "        \"data_quality_score\": locals().get('data_quality_score', 1.0)     # Update based on your variables\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Notebook Success:\")\n",
    "    print(json.dumps(success_result, indent=2))\n",
    "    \n",
    "    # Exit with success status for Airflow\n",
    "    dbutils.notebook.exit(success_result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # If any error occurs, report failure\n",
    "    failure_result = {\n",
    "        \"status\": \"FAILED\", \n",
    "        \"message\": f\"Notebook execution failed: {str(e)}\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"error_type\": type(e).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"❌ Notebook Failure:\")\n",
    "    print(json.dumps(failure_result, indent=2))\n",
    "    \n",
    "    # Exit with failure status for Airflow\n",
    "    dbutils.notebook.exit(failure_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Event_Hub_News_Consumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}