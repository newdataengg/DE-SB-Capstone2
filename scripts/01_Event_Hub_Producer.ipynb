{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ab860dd-c664-4f62-8304-b3b86a63a1e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Airflow Integration Parameters\n",
    "\n",
    "try:\n",
    "    # Create widgets for Airflow parameters\n",
    "    dbutils.widgets.text(\"batch_id\", \"manual_run\", \"Batch ID from Airflow\")\n",
    "    dbutils.widgets.text(\"execution_date\", \"\", \"Execution Date from Airflow\") \n",
    "    dbutils.widgets.text(\"force_refresh\", \"false\", \"Force data refresh\")\n",
    "    dbutils.widgets.text(\"quality_threshold\", \"0.8\", \"Data quality threshold\")\n",
    "    dbutils.widgets.text(\"dag_run_id\", \"\", \"DAG Run ID\")\n",
    "    \n",
    "    # Get parameter values\n",
    "    batch_id = dbutils.widgets.get(\"batch_id\")\n",
    "    execution_date = dbutils.widgets.get(\"execution_date\")\n",
    "    force_refresh = dbutils.widgets.get(\"force_refresh\").lower() == \"true\"\n",
    "    quality_threshold = float(dbutils.widgets.get(\"quality_threshold\"))\n",
    "    dag_run_id = dbutils.widgets.get(\"dag_run_id\")\n",
    "    \n",
    "    print(f\"\uD83C\uDFAF Airflow Parameters:\")\n",
    "    print(f\"   Batch ID: {batch_id}\")\n",
    "    print(f\"   Execution Date: {execution_date}\")\n",
    "    print(f\"   Force Refresh: {force_refresh}\")\n",
    "    print(f\"   Quality Threshold: {quality_threshold}\")\n",
    "    print(f\"   DAG Run ID: {dag_run_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Widget creation failed (normal in some contexts): {e}\")\n",
    "    # Fallback values for manual runs\n",
    "    batch_id = \"manual_run\"\n",
    "    execution_date = \"\"\n",
    "    force_refresh = False\n",
    "    quality_threshold = 0.8\n",
    "    dag_run_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4f0888-3f0d-4ba2-a943-ebcd48785026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newsapi-python\n  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (2.32.2)\nRequirement already satisfied: azure-eventhub in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (5.11.4)\nRequirement already satisfied: websocket-client in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (1.8.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests) (2024.6.2)\nRequirement already satisfied: azure-core<2.0.0,>=1.14.0 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (1.31.0)\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (4.11.0)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.16.0)\nDownloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\nInstalling collected packages: newsapi-python\nSuccessfully installed newsapi-python-0.2.7\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n\uD83D\uDD27 Applying SSL compatibility fix...\n⚠️ SSL fix warning: module 'ssl' has no attribute 'wrap_socket'\n✅ Libraries imported successfully\n\uD83D\uDD0D Connection string format: Endpoint=sb://eh-stock-sentiment-2025.servicebus.w...\n✅ Added EntityPath: stock-data-hub\n✅ All credentials retrieved from Key Vault\n✅ Event Hub connection string configured with EntityPath: stock-data-hub\n✅ Configuration complete\n\uD83C\uDFD7️ Setting up database schema with ALL consistent data types...\n\uD83D\uDCC1 Current catalog: databricks_stock_sentiment_canada\n✅ Schema databricks_stock_sentiment_canada.bronze created/verified\n\uD83D\uDDD1️ Dropped any existing tables\n✅ Table databricks_stock_sentiment_canada.bronze.stock_data created with ALL STRING types\n✅ Table databricks_stock_sentiment_canada.bronze.news_data created with ALL STRING types\n\uD83D\uDCCB Available tables in bronze schema: ['news_data', 'stock_data']\n\uD83D\uDE80 Starting Data Pipeline...\n\uD83D\uDCCA Phase 1: API Data Collection\n\uD83D\uDCC8 Fetching stock data for: AAPL, GOOGL, MSFT, AMZN, META, TSLA\n\uD83D\uDD04 Fetching stock data for 6 symbols...\n✅ Retrieved AAPL: $211.18 (Volume: 48,974,591.0)\n⏳ Waiting 12 seconds for rate limiting...\n✅ Retrieved GOOGL: $185.06 (Volume: 34,014,509.0)\n⏳ Waiting 12 seconds for rate limiting...\n✅ Retrieved MSFT: $510.05 (Volume: 21,209,666.0)\n⏳ Waiting 12 seconds for rate limiting...\n✅ Retrieved AMZN: $226.13 (Volume: 37,833,807.0)\n⏳ Waiting 12 seconds for rate limiting...\n✅ Retrieved META: $704.28 (Volume: 12,779,752.0)\n⏳ Waiting 12 seconds for rate limiting...\n✅ Retrieved TSLA: $329.65 (Volume: 94,254,993.0)\n\uD83D\uDCF0 Fetching financial news...\n\uD83D\uDD04 Fetching news articles for query: 'stock market financial'\n✅ Retrieved 8 news articles\n\n\uD83D\uDCCA Data Collection Summary:\n   Stock records: 6\n   News records: 8\n\n\uD83D\uDCE1 Phase 2: Streaming to Event Hubs\n\uD83D\uDD0C Trying WebSocket connection...\n✅ Connected to Event Hub using WebSocket\n\uD83D\uDCC8 Streaming 6 stock records...\n✅ Sent 6 stock_price events to Event Hub\n\uD83D\uDCF0 Streaming 8 news records...\n✅ Sent 8 news_sentiment events to Event Hub\n✅ Event Hub connection closed\n✅ Successfully streamed all data to Event Hubs\n\n\uD83D\uDCBE Phase 3: Backup to Bronze Layer\n✅ Saved 6 stock records to Unity Catalog\n✅ Also backed up stock data to ADLS2\n✅ Saved 8 news records to Unity Catalog\n✅ Also backed up news data to ADLS2\n\n\uD83C\uDF89 Pipeline Complete!\n✅ Processed: 6 stocks, 8 news articles\n\uD83D\uDCE1 Data streamed to Event Hubs for real-time processing\n\uD83D\uDCBE Data backed up to Bronze layer for reliability\n\n\uD83D\uDD0D Bronze Layer Verification:\n   Total stock records: 6\n   Total news records: 8\n\n\uD83D\uDCCA Recent Stock Data:\n+------+-----------+----------+--------------------+----------------+\n|symbol|close_price|    volume|      ingestion_time|ingestion_source|\n+------+-----------+----------+--------------------+----------------+\n|  MSFT|     510.05|21209666.0|2025-07-21T22:10:...|        producer|\n| GOOGL|     185.06|34014509.0|2025-07-21T22:10:...|        producer|\n|  AMZN|     226.13|37833807.0|2025-07-21T22:10:...|        producer|\n|  AAPL|     211.18|48974591.0|2025-07-21T22:10:...|        producer|\n|  META|     704.28|12779752.0|2025-07-21T22:10:...|        producer|\n+------+-----------+----------+--------------------+----------------+\n\n\n\uD83D\uDCF0 Recent News Data:\n+-------------------------------------------------------------------------+---------------+--------------------------------+------------------+----------------+\n|title                                                                    |sentiment_score|ingestion_time                  |sentiment_category|ingestion_source|\n+-------------------------------------------------------------------------+---------------+--------------------------------+------------------+----------------+\n|Stock market today: Live updates                                         |0.1            |2025-07-21T22:10:53.101403+00:00|neutral           |producer        |\n|Is It Time To Consider Buying Retail Food Group Limited (ASX:RFG)?       |0.2            |2025-07-21T22:10:53.101403+00:00|positive          |producer        |\n|Your Photography Skills Are Already Obsolete (You Just Don't Know It Yet)|0.0            |2025-07-21T22:10:53.101403+00:00|neutral           |producer        |\n+-------------------------------------------------------------------------+---------------+--------------------------------+------------------+----------------+\n\n\uD83D\uDD04 Event Hub Producer Complete!\n\uD83D\uDCCB Stock Table: databricks_stock_sentiment_canada.bronze.stock_data\n\uD83D\uDCCB News Table: databricks_stock_sentiment_canada.bronze.news_data\n\n⏰ Execution completed: 2025-07-21 22:11:01\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %md\n",
    "# MAGIC # Event Hub Producer\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Install required libraries\n",
    "%pip install newsapi-python requests azure-eventhub websocket-client\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Import libraries and handle SSL compatibility\n",
    "import requests\n",
    "import json\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from azure.eventhub import EventHubProducerClient, EventData, TransportType\n",
    "from azure.eventhub.exceptions import EventHubError\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from newsapi import NewsApiClient\n",
    "import time\n",
    "\n",
    "# Fix SSL compatibility issue for Event Hub in Databricks\n",
    "try:\n",
    "    import ssl\n",
    "    if not hasattr(ssl, 'wrap_socket'):\n",
    "        print(\"\uD83D\uDD27 Applying SSL compatibility fix...\")\n",
    "        original_wrap_socket = ssl.wrap_socket\n",
    "        def wrap_socket_fix(sock, **kwargs):\n",
    "            context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
    "            return context.wrap_socket(sock, **kwargs)\n",
    "        ssl.wrap_socket = wrap_socket_fix\n",
    "        print(\"✅ SSL compatibility fix applied\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ SSL fix warning: {e}\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration - Get credentials from Key Vault\n",
    "try:\n",
    "    polygon_api_key = dbutils.secrets.get(scope=\"stock-project\", key=\"polygon-api-key\")\n",
    "    newsapi_key = dbutils.secrets.get(scope=\"stock-project\", key=\"newsapi-key\")\n",
    "    \n",
    "    eh_connection_string = dbutils.secrets.get(scope=\"stock-project\", key=\"event-hub-connection-string\")\n",
    "    \n",
    "    # Debug: Print connection string format (safely)\n",
    "    print(f\"\uD83D\uDD0D Connection string format: {eh_connection_string[:50]}...\")\n",
    "    \n",
    "    # Check if EntityPath is in connection string, if not add it\n",
    "    if \"EntityPath=\" not in eh_connection_string:\n",
    "        event_hub_name = \"stock-data-hub\"\n",
    "        eh_connection_string = f\"{eh_connection_string};EntityPath={event_hub_name}\"\n",
    "        print(f\"✅ Added EntityPath: {event_hub_name}\")\n",
    "    else:\n",
    "        entity_path_part = [part for part in eh_connection_string.split(';') if part.startswith('EntityPath=')]\n",
    "        if entity_path_part:\n",
    "            event_hub_name = entity_path_part[0].split('=')[1]\n",
    "            print(f\"✅ Found Event Hub name in connection string: {event_hub_name}\")\n",
    "    \n",
    "    storage_account_key = dbutils.secrets.get(scope=\"stock-project\", key=\"storage-account-key\")\n",
    "    print(\"✅ All credentials retrieved from Key Vault\")\n",
    "    print(f\"✅ Event Hub connection string configured with EntityPath: {event_hub_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error retrieving secrets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Storage configuration\n",
    "storage_account_name = \"dlsstocksentiment2025\"\n",
    "container_name = \"data\"\n",
    "spark.conf.set(f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", storage_account_key)\n",
    "\n",
    "adls_base_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\"\n",
    "bronze_stock_path = f\"{adls_base_path}/bronze/stock_data\"\n",
    "bronze_news_path = f\"{adls_base_path}/bronze/news_data\"\n",
    "\n",
    "print(\"✅ Configuration complete\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Schema and Database Creation\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def create_schemas_and_tables():\n",
    "    \"\"\"Create catalog, schema, and tables\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"\uD83C\uDFD7️ Setting up database schema with ALL consistent data types...\")\n",
    "        \n",
    "        # Get current catalog name\n",
    "        current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "        print(f\"\uD83D\uDCC1 Current catalog: {current_catalog}\")\n",
    "        \n",
    "        # Create bronze schema if it doesn't exist\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {current_catalog}.bronze\")\n",
    "        print(f\"✅ Schema {current_catalog}.bronze created/verified\")\n",
    "        \n",
    "        # Create COMPLETELY NEW TABLES with unique names to avoid ALL conflicts\n",
    "        new_stock_table = f\"{current_catalog}.bronze.stock_data\"\n",
    "        new_news_table = f\"{current_catalog}.bronze.news_data\"\n",
    "        \n",
    "        # Drop any existing tables\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {new_stock_table}\")\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {new_news_table}\")\n",
    "            print(\"\uD83D\uDDD1️ Dropped any existing tables\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Create stock_data table with ALL STRING types to eliminate conflicts\n",
    "        stock_table_ddl = f\"\"\"\n",
    "        CREATE TABLE {new_stock_table} (\n",
    "            symbol STRING,\n",
    "            timestamp STRING,\n",
    "            open_price STRING,  -- STRING to avoid precision conflicts\n",
    "            high_price STRING,  -- STRING to avoid precision conflicts\n",
    "            low_price STRING,   -- STRING to avoid precision conflicts\n",
    "            close_price STRING, -- STRING to avoid precision conflicts\n",
    "            volume STRING,      -- STRING to avoid type conflicts\n",
    "            source STRING,\n",
    "            data_type STRING,\n",
    "            market_cap_estimate STRING,  -- STRING to avoid precision conflicts\n",
    "            daily_range STRING,          -- STRING to avoid precision conflicts\n",
    "            api_fetch_time STRING,\n",
    "            ingestion_time STRING,\n",
    "            processed_date STRING,\n",
    "            ingestion_batch STRING,\n",
    "            ingestion_source STRING,\n",
    "            stream_attempted STRING      -- STRING instead of BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(stock_table_ddl)\n",
    "        print(f\"✅ Table {new_stock_table} created with ALL STRING types\")\n",
    "        \n",
    "        # Create news_data table with ALL STRING types to eliminate conflicts\n",
    "        news_table_ddl = f\"\"\"\n",
    "        CREATE TABLE {new_news_table} (\n",
    "            title STRING,\n",
    "            content STRING,\n",
    "            source STRING,\n",
    "            published_at STRING,\n",
    "            url STRING,\n",
    "            sentiment_score STRING,     -- STRING to avoid precision conflicts\n",
    "            data_type STRING,\n",
    "            title_length STRING,        -- STRING to avoid INT/LONG conflicts\n",
    "            content_length STRING,      -- STRING to avoid INT/LONG conflicts\n",
    "            api_fetch_time STRING,\n",
    "            sentiment_category STRING,\n",
    "            ingestion_time STRING,\n",
    "            processed_date STRING,\n",
    "            ingestion_batch STRING,\n",
    "            ingestion_source STRING,\n",
    "            stream_attempted STRING     -- STRING instead of BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        \"\"\"\n",
    "        \n",
    "        spark.sql(news_table_ddl)\n",
    "        print(f\"✅ Table {new_news_table} created with ALL STRING types\")\n",
    "        \n",
    "        # Verify tables exist\n",
    "        tables = spark.sql(f\"SHOW TABLES IN {current_catalog}.bronze\").collect()\n",
    "        table_names = [row.tableName for row in tables]\n",
    "        print(f\"\uD83D\uDCCB Available tables in bronze schema: {table_names}\")\n",
    "        \n",
    "        return current_catalog, new_stock_table, new_news_table\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating schemas: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Create schemas and get catalog name\n",
    "current_catalog, stock_table_name, news_table_name = create_schemas_and_tables()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Event Hub Producer Functions\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class EventHubProducer:\n",
    "    def __init__(self, connection_string):\n",
    "        self.connection_string = connection_string\n",
    "        self.client = None\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Initialize Event Hub producer client\"\"\"\n",
    "        connection_methods = [\n",
    "            (\"WebSocket\", TransportType.AmqpOverWebsocket),\n",
    "            (\"Auto-detect\", None),\n",
    "            (\"Standard AMQP\", TransportType.Amqp)\n",
    "        ]\n",
    "        \n",
    "        for method_name, transport_type in connection_methods:\n",
    "            try:\n",
    "                print(f\"\uD83D\uDD0C Trying {method_name} connection...\")\n",
    "                \n",
    "                if transport_type:\n",
    "                    self.client = EventHubProducerClient.from_connection_string(\n",
    "                        self.connection_string,\n",
    "                        transport_type=transport_type\n",
    "                    )\n",
    "                else:\n",
    "                    self.client = EventHubProducerClient.from_connection_string(\n",
    "                        self.connection_string\n",
    "                    )\n",
    "                \n",
    "                # Test the connection by creating a batch\n",
    "                test_batch = self.client.create_batch()\n",
    "                \n",
    "                print(f\"✅ Connected to Event Hub using {method_name}\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {method_name} failed: {str(e)[:60]}...\")\n",
    "                if self.client:\n",
    "                    try:\n",
    "                        self.client.close()\n",
    "                    except:\n",
    "                        pass\n",
    "                    self.client = None\n",
    "                continue\n",
    "        \n",
    "        print(\"❌ All connection methods failed\")\n",
    "        return False\n",
    "    \n",
    "    def send_batch_data(self, data_records, data_type):\n",
    "        \"\"\"Send batch of data to Event Hub\"\"\"\n",
    "        if not self.client:\n",
    "            print(\"❌ Event Hub client not connected\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            event_data_batch = self.client.create_batch()\n",
    "            sent_count = 0\n",
    "            \n",
    "            for record in data_records:\n",
    "                # Add event metadata\n",
    "                enhanced_record = record.copy()\n",
    "                enhanced_record.update({\n",
    "                    'event_timestamp': datetime.now(timezone.utc).isoformat(),\n",
    "                    'data_type': data_type,\n",
    "                    'event_source': 'api_producer',\n",
    "                    'partition_key': record.get('symbol', 'general'),\n",
    "                    'processing_mode': 'streaming'\n",
    "                })\n",
    "                \n",
    "                # Create event data\n",
    "                event_data = EventData(json.dumps(enhanced_record))\n",
    "                \n",
    "                # Set partition key for better distribution\n",
    "                if 'symbol' in record:\n",
    "                    event_data.properties = {'partition_key': record['symbol']}\n",
    "                \n",
    "                try:\n",
    "                    event_data_batch.add(event_data)\n",
    "                    sent_count += 1\n",
    "                except ValueError:\n",
    "                    # Batch is full, send it and create a new one\n",
    "                    self.client.send_batch(event_data_batch)\n",
    "                    event_data_batch = self.client.create_batch()\n",
    "                    event_data_batch.add(event_data)\n",
    "                    sent_count += 1\n",
    "            \n",
    "            # Send remaining events\n",
    "            if event_data_batch:\n",
    "                self.client.send_batch(event_data_batch)\n",
    "            \n",
    "            print(f\"✅ Sent {sent_count} {data_type} events to Event Hub\")\n",
    "            return True\n",
    "            \n",
    "        except EventHubError as e:\n",
    "            print(f\"❌ Event Hub error: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Unexpected error sending to Event Hub: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Event Hub connection\"\"\"\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            print(\"✅ Event Hub connection closed\")\n",
    "\n",
    "# Initialize Event Hub producer\n",
    "eh_producer = EventHubProducer(eh_connection_string)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Enhanced API Data Fetching (ALL DATA TYPES AS STRINGS)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_stock_data_all_strings(symbols):\n",
    "    \"\"\"Fetch stock data with ALL data as strings to avoid type conflicts\"\"\"\n",
    "    stock_data = []\n",
    "    \n",
    "    print(f\"\uD83D\uDD04 Fetching stock data for {len(symbols)} symbols...\")\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            url = f\"https://api.polygon.io/v2/aggs/ticker/{symbol}/prev\"\n",
    "            params = {\"apikey\": polygon_api_key}\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'results' in data and len(data['results']) > 0:\n",
    "                result = data['results'][0]\n",
    "                \n",
    "                # Convert timestamp to string consistently\n",
    "                trade_timestamp = datetime.fromtimestamp(result['t'] / 1000)\n",
    "                \n",
    "                # ALL VALUES AS STRINGS to eliminate type conflicts\n",
    "                stock_record = {\n",
    "                    \"symbol\": str(symbol),\n",
    "                    \"timestamp\": str(trade_timestamp.isoformat()),\n",
    "                    \"open_price\": str(result['o']),      # STRING\n",
    "                    \"high_price\": str(result['h']),     # STRING\n",
    "                    \"low_price\": str(result['l']),      # STRING\n",
    "                    \"close_price\": str(result['c']),    # STRING\n",
    "                    \"volume\": str(result['v']),         # STRING\n",
    "                    \"source\": str(\"polygon.io\"),\n",
    "                    \"data_type\": str(\"stock_price\"),\n",
    "                    \"market_cap_estimate\": str(float(result['c']) * int(result['v'])),  # STRING\n",
    "                    \"daily_range\": str(float(result['h']) - float(result['l'])),       # STRING\n",
    "                    \"api_fetch_time\": str(datetime.now(timezone.utc).isoformat())\n",
    "                }\n",
    "                \n",
    "                stock_data.append(stock_record)\n",
    "                print(f\"✅ Retrieved {symbol}: ${result['c']} (Volume: {result['v']:,})\")\n",
    "            else:\n",
    "                print(f\"⚠️ No data available for {symbol}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching {symbol}: {e}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        if symbol != symbols[-1]:\n",
    "            print(\"⏳ Waiting 12 seconds for rate limiting...\")\n",
    "            time.sleep(12)\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def get_news_data_all_strings(query=\"stock market\", page_size=10):\n",
    "    \"\"\"Fetch news data with ALL data as strings to avoid type conflicts\"\"\"\n",
    "    try:\n",
    "        print(f\"\uD83D\uDD04 Fetching news articles for query: '{query}'\")\n",
    "        \n",
    "        newsapi = NewsApiClient(api_key=newsapi_key)\n",
    "        \n",
    "        from_date = datetime.now() - timedelta(days=1)\n",
    "        \n",
    "        articles = newsapi.get_everything(\n",
    "            q=query,\n",
    "            from_param=from_date.strftime('%Y-%m-%d'),\n",
    "            language='en',\n",
    "            sort_by='publishedAt',\n",
    "            page_size=page_size\n",
    "        )\n",
    "        \n",
    "        news_data = []\n",
    "        \n",
    "        for article in articles['articles']:\n",
    "            if article['title'] and (article['content'] or article['description']):\n",
    "                content = article['content'] or article['description'] or \"\"\n",
    "                \n",
    "                # Enhanced sentiment scoring\n",
    "                positive_words = ['surge', 'rise', 'gain', 'profit', 'growth', 'bullish', 'positive', 'up', 'increase', 'strong']\n",
    "                negative_words = ['fall', 'drop', 'loss', 'decline', 'bearish', 'negative', 'crash', 'down', 'decrease', 'weak']\n",
    "                \n",
    "                title_lower = article['title'].lower()\n",
    "                content_lower = content.lower()\n",
    "                \n",
    "                sentiment_score = 0.0\n",
    "                for word in positive_words:\n",
    "                    if word in title_lower or word in content_lower:\n",
    "                        sentiment_score += 0.1\n",
    "                \n",
    "                for word in negative_words:\n",
    "                    if word in title_lower or word in content_lower:\n",
    "                        sentiment_score -= 0.1\n",
    "                \n",
    "                # Clamp sentiment score\n",
    "                if sentiment_score > 1.0:\n",
    "                    sentiment_score = 1.0\n",
    "                elif sentiment_score < -1.0:\n",
    "                    sentiment_score = -1.0\n",
    "                \n",
    "                # Round sentiment score\n",
    "                sentiment_score_rounded = float(int(sentiment_score * 100) / 100)\n",
    "                \n",
    "                # Determine sentiment category\n",
    "                if sentiment_score_rounded > 0.1:\n",
    "                    sentiment_cat = \"positive\"\n",
    "                elif sentiment_score_rounded < -0.1:\n",
    "                    sentiment_cat = \"negative\"\n",
    "                else:\n",
    "                    sentiment_cat = \"neutral\"\n",
    "                \n",
    "                # ALL VALUES AS STRINGS to eliminate type conflicts\n",
    "                news_record = {\n",
    "                    \"title\": str(article['title'][:200]) if article['title'] else \"\",\n",
    "                    \"content\": str(content[:500]),\n",
    "                    \"source\": str(article['source']['name']) if article['source']['name'] else \"\",\n",
    "                    \"published_at\": str(article['publishedAt']) if article['publishedAt'] else \"\",\n",
    "                    \"url\": str(article['url']) if article['url'] else \"\",\n",
    "                    \"sentiment_score\": str(sentiment_score_rounded),    # STRING\n",
    "                    \"data_type\": str(\"news_sentiment\"),\n",
    "                    \"title_length\": str(len(article['title'])) if article['title'] else \"0\",     # STRING\n",
    "                    \"content_length\": str(len(content)),                                         # STRING\n",
    "                    \"api_fetch_time\": str(datetime.now(timezone.utc).isoformat()),\n",
    "                    \"sentiment_category\": str(sentiment_cat)\n",
    "                }\n",
    "                \n",
    "                news_data.append(news_record)\n",
    "        \n",
    "        print(f\"✅ Retrieved {len(news_data)} news articles\")\n",
    "        return news_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching news: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Hybrid Data Pipeline\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"Main pipeline: API → Event Hubs → Bronze Backup\"\"\"\n",
    "    \n",
    "    print(\"\uD83D\uDE80 Starting Data Pipeline...\")\n",
    "    print(\"\uD83D\uDCCA Phase 1: API Data Collection\")\n",
    "    \n",
    "    # Define stock symbols\n",
    "    stock_symbols = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"META\", \"TSLA\"]\n",
    "    \n",
    "    # Fetch data from APIs\n",
    "    print(f\"\uD83D\uDCC8 Fetching stock data for: {', '.join(stock_symbols)}\")\n",
    "    stock_data = get_stock_data_all_strings(stock_symbols)\n",
    "    \n",
    "    print(f\"\uD83D\uDCF0 Fetching financial news...\")\n",
    "    news_data = get_news_data_all_strings(\"stock market financial\", 8)\n",
    "    \n",
    "    print(f\"\\n\uD83D\uDCCA Data Collection Summary:\")\n",
    "    print(f\"   Stock records: {len(stock_data)}\")\n",
    "    print(f\"   News records: {len(news_data)}\")\n",
    "    \n",
    "    # Phase 2: Stream to Event Hubs\n",
    "    print(f\"\\n\uD83D\uDCE1 Phase 2: Streaming to Event Hubs\")\n",
    "    \n",
    "    if eh_producer.connect():\n",
    "        streaming_success = True\n",
    "        \n",
    "        # Send stock data\n",
    "        if stock_data:\n",
    "            print(f\"\uD83D\uDCC8 Streaming {len(stock_data)} stock records...\")\n",
    "            if not eh_producer.send_batch_data(stock_data, \"stock_price\"):\n",
    "                streaming_success = False\n",
    "        \n",
    "        # Send news data  \n",
    "        if news_data:\n",
    "            print(f\"\uD83D\uDCF0 Streaming {len(news_data)} news records...\")\n",
    "            if not eh_producer.send_batch_data(news_data, \"news_sentiment\"):\n",
    "                streaming_success = False\n",
    "        \n",
    "        eh_producer.close()\n",
    "        \n",
    "        if streaming_success:\n",
    "            print(\"✅ Successfully streamed all data to Event Hubs\")\n",
    "        else:\n",
    "            print(\"⚠️ Some data failed to stream - proceeding with backup\")\n",
    "    else:\n",
    "        print(\"⚠️ Event Hub connection failed - proceeding with direct Bronze save\")\n",
    "        streaming_success = False\n",
    "    \n",
    "    # Phase 3: Backup to Bronze Layer\n",
    "    print(f\"\\n\uD83D\uDCBE Phase 3: Backup to Bronze Layer\")\n",
    "    save_to_bronze_all_strings(stock_data, news_data)\n",
    "    \n",
    "    return len(stock_data), len(news_data)\n",
    "\n",
    "def save_to_bronze_all_strings(stock_data, news_data):\n",
    "    \"\"\"Save data directly to Bronze layer\"\"\"\n",
    "    \n",
    "    if not current_catalog:\n",
    "        print(\"❌ Schema not available - cannot save to Bronze\")\n",
    "        return\n",
    "    \n",
    "    batch_id = f\"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    current_time_str = datetime.now(timezone.utc).isoformat()\n",
    "    current_date_str = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Save stock data\n",
    "    if stock_data:\n",
    "        try:\n",
    "            # Add metadata as strings\n",
    "            for record in stock_data:\n",
    "                record[\"ingestion_time\"] = str(current_time_str)\n",
    "                record[\"processed_date\"] = str(current_date_str)\n",
    "                record[\"ingestion_batch\"] = str(batch_id)\n",
    "                record[\"ingestion_source\"] = str(\"producer\")\n",
    "                record[\"stream_attempted\"] = str(True)  # STRING instead of boolean\n",
    "            \n",
    "            stock_df = spark.createDataFrame(stock_data)\n",
    "            \n",
    "            # Save to Unity Catalog (managed tables) - NO SCHEMA MERGING\n",
    "            (stock_df.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"append\")\n",
    "             .option(\"mergeSchema\", \"false\")\n",
    "             .saveAsTable(stock_table_name))\n",
    "            \n",
    "            print(f\"✅ Saved {len(stock_data)} stock records to Unity Catalog\")\n",
    "            \n",
    "            # Try to backup to ADLS2\n",
    "            try:\n",
    "                (stock_df.write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"append\")\n",
    "                 .option(\"mergeSchema\", \"false\")\n",
    "                 .save(bronze_stock_path))\n",
    "                print(f\"✅ Also backed up stock data to ADLS2\")\n",
    "            except Exception as adls_error:\n",
    "                print(f\"⚠️ ADLS2 backup failed: {str(adls_error)[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving stock backup: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    # Save news data\n",
    "    if news_data:\n",
    "        try:\n",
    "            # Add metadata as strings\n",
    "            for record in news_data:\n",
    "                record[\"ingestion_time\"] = str(current_time_str)\n",
    "                record[\"processed_date\"] = str(current_date_str)\n",
    "                record[\"ingestion_batch\"] = str(batch_id)\n",
    "                record[\"ingestion_source\"] = str(\"producer\")\n",
    "                record[\"stream_attempted\"] = str(True)  # STRING instead of boolean\n",
    "            \n",
    "            news_df = spark.createDataFrame(news_data)\n",
    "            \n",
    "            # Save to Unity Catalog (managed tables) - NO SCHEMA MERGING\n",
    "            (news_df.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"append\")\n",
    "             .option(\"mergeSchema\", \"false\")\n",
    "             .saveAsTable(news_table_name))\n",
    "            \n",
    "            print(f\"✅ Saved {len(news_data)} news records to Unity Catalog\")\n",
    "            \n",
    "            # Try to backup to ADLS2\n",
    "            try:\n",
    "                (news_df.write\n",
    "                 .format(\"delta\")\n",
    "                 .mode(\"append\")\n",
    "                 .option(\"mergeSchema\", \"false\")\n",
    "                 .save(bronze_news_path))\n",
    "                print(f\"✅ Also backed up news data to ADLS2\")\n",
    "            except Exception as adls_error:\n",
    "                print(f\"⚠️ ADLS2 backup failed: {str(adls_error)[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving news backup: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Execute Pipeline\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run the pipeline\n",
    "try:\n",
    "    stock_count, news_count = run_data_pipeline()\n",
    "    \n",
    "    print(f\"\\n\uD83C\uDF89 Pipeline Complete!\")\n",
    "    print(f\"✅ Processed: {stock_count} stocks, {news_count} news articles\")\n",
    "    print(f\"\uD83D\uDCE1 Data streamed to Event Hubs for real-time processing\")\n",
    "    print(f\"\uD83D\uDCBE Data backed up to Bronze layer for reliability\")\n",
    "    \n",
    "    # Verify bronze layer\n",
    "    print(f\"\\n\uD83D\uDD0D Bronze Layer Verification:\")\n",
    "    \n",
    "    try:\n",
    "        if stock_table_name and news_table_name:\n",
    "            total_stock = spark.table(stock_table_name).count()\n",
    "            total_news = spark.table(news_table_name).count()\n",
    "            \n",
    "            print(f\"   Total stock records: {total_stock}\")\n",
    "            print(f\"   Total news records: {total_news}\")\n",
    "            \n",
    "            # Show recent data\n",
    "            if total_stock > 0:\n",
    "                print(f\"\\n\uD83D\uDCCA Recent Stock Data:\")\n",
    "                (spark.table(stock_table_name)\n",
    "                 .select(\"symbol\", \"close_price\", \"volume\", \"ingestion_time\", \"ingestion_source\")\n",
    "                 .orderBy(col(\"ingestion_time\").desc())\n",
    "                 .limit(5)\n",
    "                 .show())\n",
    "            \n",
    "            if total_news > 0:\n",
    "                print(f\"\\n\uD83D\uDCF0 Recent News Data:\")\n",
    "                (spark.table(news_table_name)\n",
    "                 .select(\"title\", \"sentiment_score\", \"ingestion_time\", \"sentiment_category\", \"ingestion_source\")\n",
    "                 .orderBy(col(\"ingestion_time\").desc())\n",
    "                 .limit(3)\n",
    "                 .show(truncate=False))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in verification: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pipeline failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\uD83D\uDD04 Event Hub Producer Complete!\")\n",
    "print(f\"\uD83D\uDCCB Stock Table: {stock_table_name}\")\n",
    "print(f\"\uD83D\uDCCB News Table: {news_table_name}\")\n",
    "print(f\"\\n⏰ Execution completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "befd48c9-0db4-46fe-b0f7-591a2b819bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Airflow Integration - Success/Failure Reporting\n",
    "\n",
    "try:\n",
    "    # If we reach here, notebook executed successfully\n",
    "    success_result = {\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"message\": \"Notebook execution completed successfully\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"records_processed\": locals().get('total_records_processed', 0),  # Update based on your variables\n",
    "        \"data_quality_score\": locals().get('data_quality_score', 1.0)     # Update based on your variables\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Notebook Success:\")\n",
    "    print(json.dumps(success_result, indent=2))\n",
    "    \n",
    "    # Exit with success status for Airflow\n",
    "    dbutils.notebook.exit(success_result)\n",
    "    \n",
    "except Exception as e:\n",
    "    # If any error occurs, report failure\n",
    "    failure_result = {\n",
    "        \"status\": \"FAILED\", \n",
    "        \"message\": f\"Notebook execution failed: {str(e)}\",\n",
    "        \"batch_id\": batch_id,\n",
    "        \"execution_timestamp\": datetime.now().isoformat(),\n",
    "        \"error_type\": type(e).__name__\n",
    "    }\n",
    "    \n",
    "    print(f\"❌ Notebook Failure:\")\n",
    "    print(json.dumps(failure_result, indent=2))\n",
    "    \n",
    "    # Exit with failure status for Airflow\n",
    "    dbutils.notebook.exit(failure_result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Event_Hub_Producer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}